{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "APM598 - Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g54ZD5lommO"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN9jRhLy9Uab"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0_T4PXn9e9Q",
        "outputId": "2ccbe7db-84cc-47af-d48d-7e1bcc2e05d8"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAFORyv_-nIR"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/Projects/APM598 Project/Data/europarl-v7.fr-en.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    europarl_en = f.read()\n",
        "with open(\"/content/drive/MyDrive/Projects/APM598 Project/Data/europarl-v7.fr-en.fr\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    europarl_fr = f.read()\n",
        "with open(\"/content/drive/MyDrive/Projects/APM598 Project/Data/P85-Non-Breaking-Prefix.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    non_breaking_prefix_en = f.read()\n",
        "with open(\"/content/drive/MyDrive/Projects/APM598 Project/Data/P85-Non-Breaking-Prefix.fr\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    non_breaking_prefix_fr = f.read()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YH9erLCQ__KC",
        "outputId": "97481798-974c-4662-e229-0879f69c4890"
      },
      "source": [
        "europarl_en[:50]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Resumption of the session\\nI declare resumed the se'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7D0vWCRB0rf"
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
        "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEO38YpEDx1x"
      },
      "source": [
        "corpus_en = europarl_en\n",
        "\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
        "\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "corpus_fr = europarl_fr\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "    corpus_fr = corpus_fr.replace(prefix, prefix + '$$$')\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_fr)\n",
        "corpus_fr = re.sub(r\".\\$\\$\\$\", '', corpus_fr)\n",
        "corpus_fr = re.sub(r\"  +\", \" \", corpus_fr)\n",
        "corpus_fr = corpus_fr.split('\\n')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3DE-Yg2ADT5"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4xGcaPKASBW"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7LIyBlYLOwR"
      },
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13)\n",
        "tokenizer_fr = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_fr, target_vocab_size=2**13)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4sbInrvUDR5"
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 \n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2 "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiXXD3FtUFIY"
      },
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1]\n",
        "           for sentence in corpus_fr]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsyF07CRLU_F"
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZBtfhozMo2P"
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=MAX_LENGTH)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je3RjPs0Mq3m"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsLtnVSYPdPs"
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQGmOzNIP2of"
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    \n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "    \n",
        "    return attention"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLo77gdcP6iK"
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    \n",
        "    def __init__(self, nb_proj):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        \n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "        \n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "    def split_proj(self, inputs, batch_size): \n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.nb_proj,\n",
        "                 self.d_proj)\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) \n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) \n",
        "    \n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "        \n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "        \n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        \n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        \n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1, self.d_model))\n",
        "        \n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COZkx8CFQPG0"
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOoH4VRiQUJV"
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjPcYiaUQVjU"
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        \n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "       \n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                    activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        attention = self.multi_head_attention_1(inputs,\n",
        "                                                inputs,\n",
        "                                                inputs,\n",
        "                                                mask_1)\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "        \n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q0D8tlZQcEc"
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for i in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2,\n",
        "                                         training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLYyZxQOQgIH"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "    \n",
        "    def create_padding_mask(self, seq):\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nt_eGHNQhfZ"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "parameters\n",
        "D_MODEL = 128 \n",
        "NB_LAYERS = 4 \n",
        "FFN_UNITS = 512 \n",
        "NB_PROJ = 8 \n",
        "DROPOUT_RATE = 0.1 \n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8swIU_K_0qc"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0ZVCcG2QmV1"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAMAhcpbQqE4"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvk5uJ3eR6Us",
        "outputId": "7a265bc6-df3d-4cb1-ef2d-dc0824d07f46"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/Projects/APM598 Project/Data/ckpt\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-xS3VEnSCoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c83ac54f-f479-4409-9f0e-156570271965"
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 0.9123 Accuracy 0.5321\n",
            "Epoch 1 Batch 50 Loss 0.9376 Accuracy 0.5008\n",
            "Epoch 1 Batch 100 Loss 0.9359 Accuracy 0.5012\n",
            "Epoch 1 Batch 150 Loss 0.9397 Accuracy 0.5001\n",
            "Epoch 1 Batch 200 Loss 0.9403 Accuracy 0.4999\n",
            "Epoch 1 Batch 250 Loss 0.9441 Accuracy 0.5000\n",
            "Epoch 1 Batch 300 Loss 0.9415 Accuracy 0.4997\n",
            "Epoch 1 Batch 350 Loss 0.9426 Accuracy 0.4993\n",
            "Epoch 1 Batch 400 Loss 0.9402 Accuracy 0.4997\n",
            "Epoch 1 Batch 450 Loss 0.9380 Accuracy 0.4993\n",
            "Epoch 1 Batch 500 Loss 0.9342 Accuracy 0.4989\n",
            "Epoch 1 Batch 550 Loss 0.9327 Accuracy 0.4989\n",
            "Epoch 1 Batch 600 Loss 0.9324 Accuracy 0.4988\n",
            "Epoch 1 Batch 650 Loss 0.9330 Accuracy 0.4992\n",
            "Epoch 1 Batch 700 Loss 0.9321 Accuracy 0.4995\n",
            "Epoch 1 Batch 750 Loss 0.9319 Accuracy 0.4998\n",
            "Epoch 1 Batch 800 Loss 0.9312 Accuracy 0.5002\n",
            "Epoch 1 Batch 850 Loss 0.9304 Accuracy 0.5005\n",
            "Epoch 1 Batch 900 Loss 0.9291 Accuracy 0.5008\n",
            "Epoch 1 Batch 950 Loss 0.9287 Accuracy 0.5010\n",
            "Epoch 1 Batch 1000 Loss 0.9257 Accuracy 0.5009\n",
            "Epoch 1 Batch 1050 Loss 0.9246 Accuracy 0.5014\n",
            "Epoch 1 Batch 1100 Loss 0.9238 Accuracy 0.5014\n",
            "Epoch 1 Batch 1150 Loss 0.9227 Accuracy 0.5018\n",
            "Epoch 1 Batch 1200 Loss 0.9206 Accuracy 0.5020\n",
            "Epoch 1 Batch 1250 Loss 0.9183 Accuracy 0.5022\n",
            "Epoch 1 Batch 1300 Loss 0.9160 Accuracy 0.5028\n",
            "Epoch 1 Batch 1350 Loss 0.9141 Accuracy 0.5035\n",
            "Epoch 1 Batch 1400 Loss 0.9128 Accuracy 0.5042\n",
            "Epoch 1 Batch 1450 Loss 0.9105 Accuracy 0.5048\n",
            "Epoch 1 Batch 1500 Loss 0.9082 Accuracy 0.5053\n",
            "Epoch 1 Batch 1550 Loss 0.9051 Accuracy 0.5063\n",
            "Epoch 1 Batch 1600 Loss 0.9028 Accuracy 0.5071\n",
            "Epoch 1 Batch 1650 Loss 0.9001 Accuracy 0.5080\n",
            "Epoch 1 Batch 1700 Loss 0.8978 Accuracy 0.5087\n",
            "Epoch 1 Batch 1750 Loss 0.8957 Accuracy 0.5095\n",
            "Epoch 1 Batch 1800 Loss 0.8935 Accuracy 0.5104\n",
            "Epoch 1 Batch 1850 Loss 0.8915 Accuracy 0.5112\n",
            "Epoch 1 Batch 1900 Loss 0.8894 Accuracy 0.5120\n",
            "Epoch 1 Batch 1950 Loss 0.8871 Accuracy 0.5127\n",
            "Epoch 1 Batch 2000 Loss 0.8851 Accuracy 0.5134\n",
            "Epoch 1 Batch 2050 Loss 0.8832 Accuracy 0.5138\n",
            "Epoch 1 Batch 2100 Loss 0.8806 Accuracy 0.5141\n",
            "Epoch 1 Batch 2150 Loss 0.8779 Accuracy 0.5146\n",
            "Epoch 1 Batch 2200 Loss 0.8748 Accuracy 0.5148\n",
            "Epoch 1 Batch 2250 Loss 0.8723 Accuracy 0.5149\n",
            "Epoch 1 Batch 2300 Loss 0.8694 Accuracy 0.5153\n",
            "Epoch 1 Batch 2350 Loss 0.8665 Accuracy 0.5154\n",
            "Epoch 1 Batch 2400 Loss 0.8638 Accuracy 0.5156\n",
            "Epoch 1 Batch 2450 Loss 0.8610 Accuracy 0.5157\n",
            "Epoch 1 Batch 2500 Loss 0.8584 Accuracy 0.5161\n",
            "Epoch 1 Batch 2550 Loss 0.8556 Accuracy 0.5163\n",
            "Epoch 1 Batch 2600 Loss 0.8530 Accuracy 0.5166\n",
            "Epoch 1 Batch 2650 Loss 0.8507 Accuracy 0.5169\n",
            "Epoch 1 Batch 2700 Loss 0.8484 Accuracy 0.5172\n",
            "Epoch 1 Batch 2750 Loss 0.8466 Accuracy 0.5174\n",
            "Epoch 1 Batch 2800 Loss 0.8444 Accuracy 0.5177\n",
            "Epoch 1 Batch 2850 Loss 0.8422 Accuracy 0.5180\n",
            "Epoch 1 Batch 2900 Loss 0.8402 Accuracy 0.5183\n",
            "Epoch 1 Batch 2950 Loss 0.8383 Accuracy 0.5186\n",
            "Epoch 1 Batch 3000 Loss 0.8366 Accuracy 0.5188\n",
            "Epoch 1 Batch 3050 Loss 0.8351 Accuracy 0.5189\n",
            "Epoch 1 Batch 3100 Loss 0.8334 Accuracy 0.5192\n",
            "Epoch 1 Batch 3150 Loss 0.8314 Accuracy 0.5193\n",
            "Epoch 1 Batch 3200 Loss 0.8297 Accuracy 0.5196\n",
            "Epoch 1 Batch 3250 Loss 0.8276 Accuracy 0.5198\n",
            "Epoch 1 Batch 3300 Loss 0.8260 Accuracy 0.5200\n",
            "Epoch 1 Batch 3350 Loss 0.8241 Accuracy 0.5202\n",
            "Epoch 1 Batch 3400 Loss 0.8224 Accuracy 0.5203\n",
            "Epoch 1 Batch 3450 Loss 0.8210 Accuracy 0.5206\n",
            "Epoch 1 Batch 3500 Loss 0.8195 Accuracy 0.5209\n",
            "Epoch 1 Batch 3550 Loss 0.8180 Accuracy 0.5212\n",
            "Epoch 1 Batch 3600 Loss 0.8164 Accuracy 0.5215\n",
            "Epoch 1 Batch 3650 Loss 0.8147 Accuracy 0.5218\n",
            "Epoch 1 Batch 3700 Loss 0.8132 Accuracy 0.5221\n",
            "Epoch 1 Batch 3750 Loss 0.8119 Accuracy 0.5224\n",
            "Epoch 1 Batch 3800 Loss 0.8105 Accuracy 0.5228\n",
            "Epoch 1 Batch 3850 Loss 0.8093 Accuracy 0.5231\n",
            "Epoch 1 Batch 3900 Loss 0.8079 Accuracy 0.5234\n",
            "Epoch 1 Batch 3950 Loss 0.8065 Accuracy 0.5237\n",
            "Epoch 1 Batch 4000 Loss 0.8054 Accuracy 0.5240\n",
            "Epoch 1 Batch 4050 Loss 0.8043 Accuracy 0.5241\n",
            "Epoch 1 Batch 4100 Loss 0.8034 Accuracy 0.5244\n",
            "Epoch 1 Batch 4150 Loss 0.8032 Accuracy 0.5245\n",
            "Epoch 1 Batch 4200 Loss 0.8033 Accuracy 0.5245\n",
            "Epoch 1 Batch 4250 Loss 0.8039 Accuracy 0.5245\n",
            "Epoch 1 Batch 4300 Loss 0.8048 Accuracy 0.5244\n",
            "Epoch 1 Batch 4350 Loss 0.8057 Accuracy 0.5242\n",
            "Epoch 1 Batch 4400 Loss 0.8067 Accuracy 0.5240\n",
            "Epoch 1 Batch 4450 Loss 0.8078 Accuracy 0.5239\n",
            "Epoch 1 Batch 4500 Loss 0.8092 Accuracy 0.5237\n",
            "Epoch 1 Batch 4550 Loss 0.8105 Accuracy 0.5236\n",
            "Epoch 1 Batch 4600 Loss 0.8120 Accuracy 0.5234\n",
            "Epoch 1 Batch 4650 Loss 0.8134 Accuracy 0.5231\n",
            "Epoch 1 Batch 4700 Loss 0.8145 Accuracy 0.5230\n",
            "Epoch 1 Batch 4750 Loss 0.8159 Accuracy 0.5228\n",
            "Epoch 1 Batch 4800 Loss 0.8173 Accuracy 0.5225\n",
            "Epoch 1 Batch 4850 Loss 0.8185 Accuracy 0.5223\n",
            "Epoch 1 Batch 4900 Loss 0.8197 Accuracy 0.5221\n",
            "Epoch 1 Batch 4950 Loss 0.8211 Accuracy 0.5219\n",
            "Epoch 1 Batch 5000 Loss 0.8226 Accuracy 0.5217\n",
            "Epoch 1 Batch 5050 Loss 0.8235 Accuracy 0.5214\n",
            "Epoch 1 Batch 5100 Loss 0.8250 Accuracy 0.5212\n",
            "Epoch 1 Batch 5150 Loss 0.8264 Accuracy 0.5210\n",
            "Epoch 1 Batch 5200 Loss 0.8276 Accuracy 0.5207\n",
            "Epoch 1 Batch 5250 Loss 0.8288 Accuracy 0.5204\n",
            "Epoch 1 Batch 5300 Loss 0.8300 Accuracy 0.5201\n",
            "Epoch 1 Batch 5350 Loss 0.8312 Accuracy 0.5197\n",
            "Epoch 1 Batch 5400 Loss 0.8324 Accuracy 0.5194\n",
            "Epoch 1 Batch 5450 Loss 0.8337 Accuracy 0.5192\n",
            "Epoch 1 Batch 5500 Loss 0.8348 Accuracy 0.5189\n",
            "Epoch 1 Batch 5550 Loss 0.8358 Accuracy 0.5186\n",
            "Epoch 1 Batch 5600 Loss 0.8368 Accuracy 0.5183\n",
            "Epoch 1 Batch 5650 Loss 0.8377 Accuracy 0.5181\n",
            "Epoch 1 Batch 5700 Loss 0.8387 Accuracy 0.5179\n",
            "Saving checkpoint for epoch 1 at /content/drive/MyDrive/Projects/APM598 Project/Data/ckpt/ckpt-19\n",
            "Time taken for 1 epoch: 1601.6061956882477 secs\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 0.9257 Accuracy 0.5362\n",
            "Epoch 2 Batch 50 Loss 0.9472 Accuracy 0.5021\n",
            "Epoch 2 Batch 100 Loss 0.9435 Accuracy 0.5024\n",
            "Epoch 2 Batch 150 Loss 0.9415 Accuracy 0.5027\n",
            "Epoch 2 Batch 200 Loss 0.9380 Accuracy 0.5019\n",
            "Epoch 2 Batch 250 Loss 0.9341 Accuracy 0.5023\n",
            "Epoch 2 Batch 300 Loss 0.9330 Accuracy 0.5022\n",
            "Epoch 2 Batch 350 Loss 0.9336 Accuracy 0.5021\n",
            "Epoch 2 Batch 400 Loss 0.9315 Accuracy 0.5017\n",
            "Epoch 2 Batch 450 Loss 0.9285 Accuracy 0.5020\n",
            "Epoch 2 Batch 500 Loss 0.9287 Accuracy 0.5013\n",
            "Epoch 2 Batch 550 Loss 0.9259 Accuracy 0.5010\n",
            "Epoch 2 Batch 600 Loss 0.9273 Accuracy 0.5011\n",
            "Epoch 2 Batch 650 Loss 0.9258 Accuracy 0.5015\n",
            "Epoch 2 Batch 700 Loss 0.9263 Accuracy 0.5017\n",
            "Epoch 2 Batch 750 Loss 0.9260 Accuracy 0.5017\n",
            "Epoch 2 Batch 800 Loss 0.9250 Accuracy 0.5016\n",
            "Epoch 2 Batch 850 Loss 0.9241 Accuracy 0.5017\n",
            "Epoch 2 Batch 900 Loss 0.9225 Accuracy 0.5018\n",
            "Epoch 2 Batch 950 Loss 0.9205 Accuracy 0.5019\n",
            "Epoch 2 Batch 1000 Loss 0.9183 Accuracy 0.5017\n",
            "Epoch 2 Batch 1050 Loss 0.9184 Accuracy 0.5019\n",
            "Epoch 2 Batch 1100 Loss 0.9173 Accuracy 0.5023\n",
            "Epoch 2 Batch 1150 Loss 0.9161 Accuracy 0.5025\n",
            "Epoch 2 Batch 1200 Loss 0.9141 Accuracy 0.5028\n",
            "Epoch 2 Batch 1250 Loss 0.9120 Accuracy 0.5029\n",
            "Epoch 2 Batch 1300 Loss 0.9097 Accuracy 0.5034\n",
            "Epoch 2 Batch 1350 Loss 0.9075 Accuracy 0.5040\n",
            "Epoch 2 Batch 1400 Loss 0.9052 Accuracy 0.5046\n",
            "Epoch 2 Batch 1450 Loss 0.9027 Accuracy 0.5054\n",
            "Epoch 2 Batch 1500 Loss 0.9004 Accuracy 0.5064\n",
            "Epoch 2 Batch 1550 Loss 0.8977 Accuracy 0.5073\n",
            "Epoch 2 Batch 1600 Loss 0.8957 Accuracy 0.5083\n",
            "Epoch 2 Batch 1650 Loss 0.8936 Accuracy 0.5091\n",
            "Epoch 2 Batch 1700 Loss 0.8913 Accuracy 0.5098\n",
            "Epoch 2 Batch 1750 Loss 0.8888 Accuracy 0.5106\n",
            "Epoch 2 Batch 1800 Loss 0.8873 Accuracy 0.5113\n",
            "Epoch 2 Batch 1850 Loss 0.8856 Accuracy 0.5120\n",
            "Epoch 2 Batch 1900 Loss 0.8830 Accuracy 0.5127\n",
            "Epoch 2 Batch 1950 Loss 0.8809 Accuracy 0.5135\n",
            "Epoch 2 Batch 2000 Loss 0.8787 Accuracy 0.5140\n",
            "Epoch 2 Batch 2050 Loss 0.8769 Accuracy 0.5143\n",
            "Epoch 2 Batch 2100 Loss 0.8743 Accuracy 0.5148\n",
            "Epoch 2 Batch 2150 Loss 0.8718 Accuracy 0.5150\n",
            "Epoch 2 Batch 2200 Loss 0.8688 Accuracy 0.5152\n",
            "Epoch 2 Batch 2250 Loss 0.8661 Accuracy 0.5154\n",
            "Epoch 2 Batch 2300 Loss 0.8634 Accuracy 0.5156\n",
            "Epoch 2 Batch 2350 Loss 0.8611 Accuracy 0.5157\n",
            "Epoch 2 Batch 2400 Loss 0.8582 Accuracy 0.5159\n",
            "Epoch 2 Batch 2450 Loss 0.8556 Accuracy 0.5161\n",
            "Epoch 2 Batch 2500 Loss 0.8528 Accuracy 0.5164\n",
            "Epoch 2 Batch 2550 Loss 0.8504 Accuracy 0.5166\n",
            "Epoch 2 Batch 2600 Loss 0.8480 Accuracy 0.5170\n",
            "Epoch 2 Batch 2650 Loss 0.8456 Accuracy 0.5173\n",
            "Epoch 2 Batch 2700 Loss 0.8430 Accuracy 0.5177\n",
            "Epoch 2 Batch 2750 Loss 0.8408 Accuracy 0.5179\n",
            "Epoch 2 Batch 2800 Loss 0.8384 Accuracy 0.5182\n",
            "Epoch 2 Batch 2850 Loss 0.8363 Accuracy 0.5184\n",
            "Epoch 2 Batch 2900 Loss 0.8348 Accuracy 0.5187\n",
            "Epoch 2 Batch 2950 Loss 0.8328 Accuracy 0.5190\n",
            "Epoch 2 Batch 3000 Loss 0.8316 Accuracy 0.5192\n",
            "Epoch 2 Batch 3050 Loss 0.8297 Accuracy 0.5194\n",
            "Epoch 2 Batch 3100 Loss 0.8281 Accuracy 0.5197\n",
            "Epoch 2 Batch 3150 Loss 0.8264 Accuracy 0.5198\n",
            "Epoch 2 Batch 3200 Loss 0.8246 Accuracy 0.5200\n",
            "Epoch 2 Batch 3250 Loss 0.8226 Accuracy 0.5203\n",
            "Epoch 2 Batch 3300 Loss 0.8212 Accuracy 0.5205\n",
            "Epoch 2 Batch 3350 Loss 0.8197 Accuracy 0.5209\n",
            "Epoch 2 Batch 3400 Loss 0.8180 Accuracy 0.5212\n",
            "Epoch 2 Batch 3450 Loss 0.8160 Accuracy 0.5214\n",
            "Epoch 2 Batch 3500 Loss 0.8143 Accuracy 0.5216\n",
            "Epoch 2 Batch 3550 Loss 0.8127 Accuracy 0.5220\n",
            "Epoch 2 Batch 3600 Loss 0.8109 Accuracy 0.5222\n",
            "Epoch 2 Batch 3650 Loss 0.8094 Accuracy 0.5226\n",
            "Epoch 2 Batch 3700 Loss 0.8078 Accuracy 0.5230\n",
            "Epoch 2 Batch 3750 Loss 0.8063 Accuracy 0.5233\n",
            "Epoch 2 Batch 3800 Loss 0.8046 Accuracy 0.5236\n",
            "Epoch 2 Batch 3850 Loss 0.8033 Accuracy 0.5239\n",
            "Epoch 2 Batch 3900 Loss 0.8021 Accuracy 0.5243\n",
            "Epoch 2 Batch 3950 Loss 0.8005 Accuracy 0.5246\n",
            "Epoch 2 Batch 4000 Loss 0.7994 Accuracy 0.5248\n",
            "Epoch 2 Batch 4050 Loss 0.7985 Accuracy 0.5251\n",
            "Epoch 2 Batch 4100 Loss 0.7977 Accuracy 0.5253\n",
            "Epoch 2 Batch 4150 Loss 0.7975 Accuracy 0.5254\n",
            "Epoch 2 Batch 4200 Loss 0.7977 Accuracy 0.5253\n",
            "Epoch 2 Batch 4250 Loss 0.7984 Accuracy 0.5252\n",
            "Epoch 2 Batch 4300 Loss 0.7990 Accuracy 0.5252\n",
            "Epoch 2 Batch 4350 Loss 0.8002 Accuracy 0.5250\n",
            "Epoch 2 Batch 4400 Loss 0.8012 Accuracy 0.5249\n",
            "Epoch 2 Batch 4450 Loss 0.8023 Accuracy 0.5247\n",
            "Epoch 2 Batch 4500 Loss 0.8036 Accuracy 0.5245\n",
            "Epoch 2 Batch 4550 Loss 0.8052 Accuracy 0.5242\n",
            "Epoch 2 Batch 4600 Loss 0.8064 Accuracy 0.5241\n",
            "Epoch 2 Batch 4650 Loss 0.8075 Accuracy 0.5239\n",
            "Epoch 2 Batch 4700 Loss 0.8089 Accuracy 0.5237\n",
            "Epoch 2 Batch 4750 Loss 0.8104 Accuracy 0.5234\n",
            "Epoch 2 Batch 4800 Loss 0.8118 Accuracy 0.5232\n",
            "Epoch 2 Batch 4850 Loss 0.8132 Accuracy 0.5230\n",
            "Epoch 2 Batch 4900 Loss 0.8145 Accuracy 0.5229\n",
            "Epoch 2 Batch 4950 Loss 0.8158 Accuracy 0.5227\n",
            "Epoch 2 Batch 5000 Loss 0.8171 Accuracy 0.5224\n",
            "Epoch 2 Batch 5050 Loss 0.8183 Accuracy 0.5222\n",
            "Epoch 2 Batch 5100 Loss 0.8196 Accuracy 0.5220\n",
            "Epoch 2 Batch 5150 Loss 0.8209 Accuracy 0.5217\n",
            "Epoch 2 Batch 5200 Loss 0.8223 Accuracy 0.5215\n",
            "Epoch 2 Batch 5250 Loss 0.8234 Accuracy 0.5212\n",
            "Epoch 2 Batch 5300 Loss 0.8248 Accuracy 0.5209\n",
            "Epoch 2 Batch 5350 Loss 0.8258 Accuracy 0.5206\n",
            "Epoch 2 Batch 5400 Loss 0.8270 Accuracy 0.5203\n",
            "Epoch 2 Batch 5450 Loss 0.8280 Accuracy 0.5200\n",
            "Epoch 2 Batch 5500 Loss 0.8290 Accuracy 0.5198\n",
            "Epoch 2 Batch 5550 Loss 0.8302 Accuracy 0.5195\n",
            "Epoch 2 Batch 5600 Loss 0.8312 Accuracy 0.5193\n",
            "Epoch 2 Batch 5650 Loss 0.8322 Accuracy 0.5190\n",
            "Epoch 2 Batch 5700 Loss 0.8332 Accuracy 0.5187\n",
            "Saving checkpoint for epoch 2 at /content/drive/MyDrive/Projects/APM598 Project/Data/ckpt/ckpt-20\n",
            "Time taken for 1 epoch: 1581.9739573001862 secs\n",
            "\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.0218 Accuracy 0.4597\n",
            "Epoch 3 Batch 50 Loss 0.9610 Accuracy 0.5032\n",
            "Epoch 3 Batch 100 Loss 0.9551 Accuracy 0.5033\n",
            "Epoch 3 Batch 150 Loss 0.9435 Accuracy 0.5038\n",
            "Epoch 3 Batch 200 Loss 0.9434 Accuracy 0.5035\n",
            "Epoch 3 Batch 250 Loss 0.9397 Accuracy 0.5032\n",
            "Epoch 3 Batch 300 Loss 0.9380 Accuracy 0.5029\n",
            "Epoch 3 Batch 350 Loss 0.9360 Accuracy 0.5028\n",
            "Epoch 3 Batch 400 Loss 0.9306 Accuracy 0.5023\n",
            "Epoch 3 Batch 450 Loss 0.9309 Accuracy 0.5024\n",
            "Epoch 3 Batch 500 Loss 0.9278 Accuracy 0.5020\n",
            "Epoch 3 Batch 550 Loss 0.9260 Accuracy 0.5013\n",
            "Epoch 3 Batch 600 Loss 0.9229 Accuracy 0.5012\n",
            "Epoch 3 Batch 650 Loss 0.9207 Accuracy 0.5015\n",
            "Epoch 3 Batch 700 Loss 0.9213 Accuracy 0.5022\n",
            "Epoch 3 Batch 750 Loss 0.9196 Accuracy 0.5025\n",
            "Epoch 3 Batch 800 Loss 0.9181 Accuracy 0.5024\n",
            "Epoch 3 Batch 850 Loss 0.9177 Accuracy 0.5025\n",
            "Epoch 3 Batch 900 Loss 0.9171 Accuracy 0.5024\n",
            "Epoch 3 Batch 950 Loss 0.9163 Accuracy 0.5025\n",
            "Epoch 3 Batch 1000 Loss 0.9134 Accuracy 0.5026\n",
            "Epoch 3 Batch 1050 Loss 0.9122 Accuracy 0.5029\n",
            "Epoch 3 Batch 1100 Loss 0.9110 Accuracy 0.5032\n",
            "Epoch 3 Batch 1150 Loss 0.9103 Accuracy 0.5036\n",
            "Epoch 3 Batch 1200 Loss 0.9086 Accuracy 0.5037\n",
            "Epoch 3 Batch 1250 Loss 0.9070 Accuracy 0.5040\n",
            "Epoch 3 Batch 1300 Loss 0.9044 Accuracy 0.5044\n",
            "Epoch 3 Batch 1350 Loss 0.9023 Accuracy 0.5050\n",
            "Epoch 3 Batch 1400 Loss 0.8997 Accuracy 0.5055\n",
            "Epoch 3 Batch 1450 Loss 0.8972 Accuracy 0.5064\n",
            "Epoch 3 Batch 1500 Loss 0.8951 Accuracy 0.5071\n",
            "Epoch 3 Batch 1550 Loss 0.8926 Accuracy 0.5080\n",
            "Epoch 3 Batch 1600 Loss 0.8902 Accuracy 0.5090\n",
            "Epoch 3 Batch 1650 Loss 0.8876 Accuracy 0.5098\n",
            "Epoch 3 Batch 1700 Loss 0.8854 Accuracy 0.5105\n",
            "Epoch 3 Batch 1750 Loss 0.8828 Accuracy 0.5113\n",
            "Epoch 3 Batch 1800 Loss 0.8809 Accuracy 0.5121\n",
            "Epoch 3 Batch 1850 Loss 0.8785 Accuracy 0.5129\n",
            "Epoch 3 Batch 1900 Loss 0.8764 Accuracy 0.5137\n",
            "Epoch 3 Batch 1950 Loss 0.8747 Accuracy 0.5144\n",
            "Epoch 3 Batch 2000 Loss 0.8729 Accuracy 0.5150\n",
            "Epoch 3 Batch 2050 Loss 0.8710 Accuracy 0.5153\n",
            "Epoch 3 Batch 2100 Loss 0.8685 Accuracy 0.5156\n",
            "Epoch 3 Batch 2150 Loss 0.8660 Accuracy 0.5158\n",
            "Epoch 3 Batch 2200 Loss 0.8631 Accuracy 0.5160\n",
            "Epoch 3 Batch 2250 Loss 0.8603 Accuracy 0.5162\n",
            "Epoch 3 Batch 2300 Loss 0.8578 Accuracy 0.5164\n",
            "Epoch 3 Batch 2350 Loss 0.8550 Accuracy 0.5166\n",
            "Epoch 3 Batch 2400 Loss 0.8520 Accuracy 0.5169\n",
            "Epoch 3 Batch 2450 Loss 0.8494 Accuracy 0.5171\n",
            "Epoch 3 Batch 2500 Loss 0.8468 Accuracy 0.5173\n",
            "Epoch 3 Batch 2550 Loss 0.8444 Accuracy 0.5177\n",
            "Epoch 3 Batch 2600 Loss 0.8416 Accuracy 0.5179\n",
            "Epoch 3 Batch 2650 Loss 0.8393 Accuracy 0.5181\n",
            "Epoch 3 Batch 2700 Loss 0.8373 Accuracy 0.5185\n",
            "Epoch 3 Batch 2750 Loss 0.8352 Accuracy 0.5188\n",
            "Epoch 3 Batch 2800 Loss 0.8334 Accuracy 0.5191\n",
            "Epoch 3 Batch 2850 Loss 0.8316 Accuracy 0.5194\n",
            "Epoch 3 Batch 2900 Loss 0.8297 Accuracy 0.5197\n",
            "Epoch 3 Batch 2950 Loss 0.8279 Accuracy 0.5199\n",
            "Epoch 3 Batch 3000 Loss 0.8258 Accuracy 0.5201\n",
            "Epoch 3 Batch 3050 Loss 0.8242 Accuracy 0.5205\n",
            "Epoch 3 Batch 3100 Loss 0.8227 Accuracy 0.5207\n",
            "Epoch 3 Batch 3150 Loss 0.8208 Accuracy 0.5210\n",
            "Epoch 3 Batch 3200 Loss 0.8191 Accuracy 0.5212\n",
            "Epoch 3 Batch 3250 Loss 0.8171 Accuracy 0.5214\n",
            "Epoch 3 Batch 3300 Loss 0.8154 Accuracy 0.5216\n",
            "Epoch 3 Batch 3350 Loss 0.8134 Accuracy 0.5219\n",
            "Epoch 3 Batch 3400 Loss 0.8117 Accuracy 0.5222\n",
            "Epoch 3 Batch 3450 Loss 0.8099 Accuracy 0.5224\n",
            "Epoch 3 Batch 3500 Loss 0.8083 Accuracy 0.5228\n",
            "Epoch 3 Batch 3550 Loss 0.8067 Accuracy 0.5231\n",
            "Epoch 3 Batch 3600 Loss 0.8051 Accuracy 0.5234\n",
            "Epoch 3 Batch 3650 Loss 0.8036 Accuracy 0.5236\n",
            "Epoch 3 Batch 3700 Loss 0.8025 Accuracy 0.5239\n",
            "Epoch 3 Batch 3750 Loss 0.8010 Accuracy 0.5241\n",
            "Epoch 3 Batch 3800 Loss 0.7994 Accuracy 0.5244\n",
            "Epoch 3 Batch 3850 Loss 0.7979 Accuracy 0.5248\n",
            "Epoch 3 Batch 3900 Loss 0.7967 Accuracy 0.5251\n",
            "Epoch 3 Batch 3950 Loss 0.7956 Accuracy 0.5253\n",
            "Epoch 3 Batch 4000 Loss 0.7943 Accuracy 0.5256\n",
            "Epoch 3 Batch 4050 Loss 0.7933 Accuracy 0.5259\n",
            "Epoch 3 Batch 4100 Loss 0.7924 Accuracy 0.5261\n",
            "Epoch 3 Batch 4150 Loss 0.7921 Accuracy 0.5262\n",
            "Epoch 3 Batch 4200 Loss 0.7928 Accuracy 0.5262\n",
            "Epoch 3 Batch 4250 Loss 0.7931 Accuracy 0.5262\n",
            "Epoch 3 Batch 4300 Loss 0.7940 Accuracy 0.5260\n",
            "Epoch 3 Batch 4350 Loss 0.7951 Accuracy 0.5259\n",
            "Epoch 3 Batch 4400 Loss 0.7962 Accuracy 0.5258\n",
            "Epoch 3 Batch 4450 Loss 0.7976 Accuracy 0.5257\n",
            "Epoch 3 Batch 4500 Loss 0.7986 Accuracy 0.5254\n",
            "Epoch 3 Batch 4550 Loss 0.8000 Accuracy 0.5252\n",
            "Epoch 3 Batch 4600 Loss 0.8013 Accuracy 0.5250\n",
            "Epoch 3 Batch 4650 Loss 0.8025 Accuracy 0.5248\n",
            "Epoch 3 Batch 4700 Loss 0.8039 Accuracy 0.5246\n",
            "Epoch 3 Batch 4750 Loss 0.8054 Accuracy 0.5244\n",
            "Epoch 3 Batch 4800 Loss 0.8066 Accuracy 0.5242\n",
            "Epoch 3 Batch 4850 Loss 0.8079 Accuracy 0.5241\n",
            "Epoch 3 Batch 4900 Loss 0.8093 Accuracy 0.5239\n",
            "Epoch 3 Batch 4950 Loss 0.8105 Accuracy 0.5237\n",
            "Epoch 3 Batch 5000 Loss 0.8119 Accuracy 0.5234\n",
            "Epoch 3 Batch 5050 Loss 0.8134 Accuracy 0.5232\n",
            "Epoch 3 Batch 5100 Loss 0.8147 Accuracy 0.5229\n",
            "Epoch 3 Batch 5150 Loss 0.8160 Accuracy 0.5227\n",
            "Epoch 3 Batch 5200 Loss 0.8174 Accuracy 0.5224\n",
            "Epoch 3 Batch 5250 Loss 0.8186 Accuracy 0.5221\n",
            "Epoch 3 Batch 5300 Loss 0.8197 Accuracy 0.5218\n",
            "Epoch 3 Batch 5350 Loss 0.8209 Accuracy 0.5215\n",
            "Epoch 3 Batch 5400 Loss 0.8220 Accuracy 0.5212\n",
            "Epoch 3 Batch 5450 Loss 0.8232 Accuracy 0.5209\n",
            "Epoch 3 Batch 5500 Loss 0.8242 Accuracy 0.5206\n",
            "Epoch 3 Batch 5550 Loss 0.8253 Accuracy 0.5203\n",
            "Epoch 3 Batch 5600 Loss 0.8262 Accuracy 0.5201\n",
            "Epoch 3 Batch 5650 Loss 0.8273 Accuracy 0.5198\n",
            "Epoch 3 Batch 5700 Loss 0.8285 Accuracy 0.5195\n",
            "Saving checkpoint for epoch 3 at /content/drive/MyDrive/Projects/APM598 Project/Data/ckpt/ckpt-21\n",
            "Time taken for 1 epoch: 1583.9904623031616 secs\n",
            "\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 0.9643 Accuracy 0.4975\n",
            "Epoch 4 Batch 50 Loss 0.9083 Accuracy 0.5074\n",
            "Epoch 4 Batch 100 Loss 0.9310 Accuracy 0.5033\n",
            "Epoch 4 Batch 150 Loss 0.9304 Accuracy 0.5022\n",
            "Epoch 4 Batch 200 Loss 0.9276 Accuracy 0.5028\n",
            "Epoch 4 Batch 250 Loss 0.9236 Accuracy 0.5025\n",
            "Epoch 4 Batch 300 Loss 0.9226 Accuracy 0.5018\n",
            "Epoch 4 Batch 350 Loss 0.9217 Accuracy 0.5019\n",
            "Epoch 4 Batch 400 Loss 0.9233 Accuracy 0.5021\n",
            "Epoch 4 Batch 450 Loss 0.9198 Accuracy 0.5024\n",
            "Epoch 4 Batch 500 Loss 0.9180 Accuracy 0.5022\n",
            "Epoch 4 Batch 550 Loss 0.9168 Accuracy 0.5022\n",
            "Epoch 4 Batch 600 Loss 0.9164 Accuracy 0.5021\n",
            "Epoch 4 Batch 650 Loss 0.9173 Accuracy 0.5027\n",
            "Epoch 4 Batch 700 Loss 0.9155 Accuracy 0.5030\n",
            "Epoch 4 Batch 750 Loss 0.9128 Accuracy 0.5033\n",
            "Epoch 4 Batch 800 Loss 0.9125 Accuracy 0.5032\n",
            "Epoch 4 Batch 850 Loss 0.9108 Accuracy 0.5034\n",
            "Epoch 4 Batch 900 Loss 0.9105 Accuracy 0.5038\n",
            "Epoch 4 Batch 950 Loss 0.9089 Accuracy 0.5039\n",
            "Epoch 4 Batch 1000 Loss 0.9071 Accuracy 0.5040\n",
            "Epoch 4 Batch 1050 Loss 0.9063 Accuracy 0.5039\n",
            "Epoch 4 Batch 1100 Loss 0.9054 Accuracy 0.5039\n",
            "Epoch 4 Batch 1150 Loss 0.9044 Accuracy 0.5040\n",
            "Epoch 4 Batch 1200 Loss 0.9035 Accuracy 0.5041\n",
            "Epoch 4 Batch 1250 Loss 0.9016 Accuracy 0.5044\n",
            "Epoch 4 Batch 1300 Loss 0.8992 Accuracy 0.5051\n",
            "Epoch 4 Batch 1350 Loss 0.8972 Accuracy 0.5059\n",
            "Epoch 4 Batch 1400 Loss 0.8952 Accuracy 0.5064\n",
            "Epoch 4 Batch 1450 Loss 0.8932 Accuracy 0.5072\n",
            "Epoch 4 Batch 1500 Loss 0.8906 Accuracy 0.5078\n",
            "Epoch 4 Batch 1550 Loss 0.8878 Accuracy 0.5088\n",
            "Epoch 4 Batch 1600 Loss 0.8855 Accuracy 0.5096\n",
            "Epoch 4 Batch 1650 Loss 0.8828 Accuracy 0.5105\n",
            "Epoch 4 Batch 1700 Loss 0.8803 Accuracy 0.5112\n",
            "Epoch 4 Batch 1750 Loss 0.8781 Accuracy 0.5118\n",
            "Epoch 4 Batch 1800 Loss 0.8764 Accuracy 0.5128\n",
            "Epoch 4 Batch 1850 Loss 0.8745 Accuracy 0.5137\n",
            "Epoch 4 Batch 1900 Loss 0.8722 Accuracy 0.5143\n",
            "Epoch 4 Batch 1950 Loss 0.8700 Accuracy 0.5151\n",
            "Epoch 4 Batch 2000 Loss 0.8679 Accuracy 0.5158\n",
            "Epoch 4 Batch 2050 Loss 0.8662 Accuracy 0.5163\n",
            "Epoch 4 Batch 2100 Loss 0.8635 Accuracy 0.5168\n",
            "Epoch 4 Batch 2150 Loss 0.8614 Accuracy 0.5169\n",
            "Epoch 4 Batch 2200 Loss 0.8586 Accuracy 0.5171\n",
            "Epoch 4 Batch 2250 Loss 0.8555 Accuracy 0.5173\n",
            "Epoch 4 Batch 2300 Loss 0.8528 Accuracy 0.5175\n",
            "Epoch 4 Batch 2350 Loss 0.8505 Accuracy 0.5177\n",
            "Epoch 4 Batch 2400 Loss 0.8478 Accuracy 0.5179\n",
            "Epoch 4 Batch 2450 Loss 0.8454 Accuracy 0.5181\n",
            "Epoch 4 Batch 2500 Loss 0.8427 Accuracy 0.5183\n",
            "Epoch 4 Batch 2550 Loss 0.8400 Accuracy 0.5185\n",
            "Epoch 4 Batch 2600 Loss 0.8378 Accuracy 0.5188\n",
            "Epoch 4 Batch 2650 Loss 0.8356 Accuracy 0.5191\n",
            "Epoch 4 Batch 2700 Loss 0.8336 Accuracy 0.5194\n",
            "Epoch 4 Batch 2750 Loss 0.8312 Accuracy 0.5197\n",
            "Epoch 4 Batch 2800 Loss 0.8294 Accuracy 0.5199\n",
            "Epoch 4 Batch 2850 Loss 0.8270 Accuracy 0.5202\n",
            "Epoch 4 Batch 2900 Loss 0.8248 Accuracy 0.5205\n",
            "Epoch 4 Batch 2950 Loss 0.8229 Accuracy 0.5208\n",
            "Epoch 4 Batch 3000 Loss 0.8210 Accuracy 0.5211\n",
            "Epoch 4 Batch 3050 Loss 0.8190 Accuracy 0.5214\n",
            "Epoch 4 Batch 3100 Loss 0.8172 Accuracy 0.5216\n",
            "Epoch 4 Batch 3150 Loss 0.8157 Accuracy 0.5218\n",
            "Epoch 4 Batch 3200 Loss 0.8140 Accuracy 0.5219\n",
            "Epoch 4 Batch 3250 Loss 0.8122 Accuracy 0.5221\n",
            "Epoch 4 Batch 3300 Loss 0.8103 Accuracy 0.5224\n",
            "Epoch 4 Batch 3350 Loss 0.8084 Accuracy 0.5227\n",
            "Epoch 4 Batch 3400 Loss 0.8068 Accuracy 0.5229\n",
            "Epoch 4 Batch 3450 Loss 0.8054 Accuracy 0.5232\n",
            "Epoch 4 Batch 3500 Loss 0.8039 Accuracy 0.5234\n",
            "Epoch 4 Batch 3550 Loss 0.8023 Accuracy 0.5238\n",
            "Epoch 4 Batch 3600 Loss 0.8005 Accuracy 0.5241\n",
            "Epoch 4 Batch 3650 Loss 0.7989 Accuracy 0.5244\n",
            "Epoch 4 Batch 3700 Loss 0.7976 Accuracy 0.5247\n",
            "Epoch 4 Batch 3750 Loss 0.7963 Accuracy 0.5251\n",
            "Epoch 4 Batch 3800 Loss 0.7948 Accuracy 0.5253\n",
            "Epoch 4 Batch 3850 Loss 0.7935 Accuracy 0.5255\n",
            "Epoch 4 Batch 3900 Loss 0.7924 Accuracy 0.5258\n",
            "Epoch 4 Batch 3950 Loss 0.7912 Accuracy 0.5260\n",
            "Epoch 4 Batch 4000 Loss 0.7902 Accuracy 0.5263\n",
            "Epoch 4 Batch 4050 Loss 0.7891 Accuracy 0.5267\n",
            "Epoch 4 Batch 4100 Loss 0.7881 Accuracy 0.5269\n",
            "Epoch 4 Batch 4150 Loss 0.7877 Accuracy 0.5270\n",
            "Epoch 4 Batch 4200 Loss 0.7883 Accuracy 0.5270\n",
            "Epoch 4 Batch 4250 Loss 0.7888 Accuracy 0.5269\n",
            "Epoch 4 Batch 4300 Loss 0.7895 Accuracy 0.5268\n",
            "Epoch 4 Batch 4350 Loss 0.7905 Accuracy 0.5266\n",
            "Epoch 4 Batch 4400 Loss 0.7917 Accuracy 0.5264\n",
            "Epoch 4 Batch 4450 Loss 0.7930 Accuracy 0.5263\n",
            "Epoch 4 Batch 4500 Loss 0.7943 Accuracy 0.5260\n",
            "Epoch 4 Batch 4550 Loss 0.7955 Accuracy 0.5258\n",
            "Epoch 4 Batch 4600 Loss 0.7969 Accuracy 0.5257\n",
            "Epoch 4 Batch 4650 Loss 0.7981 Accuracy 0.5254\n",
            "Epoch 4 Batch 4700 Loss 0.7995 Accuracy 0.5253\n",
            "Epoch 4 Batch 4750 Loss 0.8009 Accuracy 0.5251\n",
            "Epoch 4 Batch 4800 Loss 0.8021 Accuracy 0.5249\n",
            "Epoch 4 Batch 4850 Loss 0.8033 Accuracy 0.5248\n",
            "Epoch 4 Batch 4900 Loss 0.8047 Accuracy 0.5245\n",
            "Epoch 4 Batch 4950 Loss 0.8059 Accuracy 0.5244\n",
            "Epoch 4 Batch 5000 Loss 0.8073 Accuracy 0.5241\n",
            "Epoch 4 Batch 5050 Loss 0.8087 Accuracy 0.5239\n",
            "Epoch 4 Batch 5100 Loss 0.8100 Accuracy 0.5237\n",
            "Epoch 4 Batch 5150 Loss 0.8112 Accuracy 0.5234\n",
            "Epoch 4 Batch 5200 Loss 0.8126 Accuracy 0.5231\n",
            "Epoch 4 Batch 5250 Loss 0.8140 Accuracy 0.5228\n",
            "Epoch 4 Batch 5300 Loss 0.8152 Accuracy 0.5225\n",
            "Epoch 4 Batch 5350 Loss 0.8163 Accuracy 0.5222\n",
            "Epoch 4 Batch 5400 Loss 0.8173 Accuracy 0.5221\n",
            "Epoch 4 Batch 5450 Loss 0.8183 Accuracy 0.5218\n",
            "Epoch 4 Batch 5500 Loss 0.8193 Accuracy 0.5215\n",
            "Epoch 4 Batch 5550 Loss 0.8204 Accuracy 0.5212\n",
            "Epoch 4 Batch 5600 Loss 0.8214 Accuracy 0.5210\n",
            "Epoch 4 Batch 5650 Loss 0.8226 Accuracy 0.5207\n",
            "Epoch 4 Batch 5700 Loss 0.8236 Accuracy 0.5204\n",
            "Saving checkpoint for epoch 4 at /content/drive/MyDrive/Projects/APM598 Project/Data/ckpt/ckpt-22\n",
            "Time taken for 1 epoch: 1592.5568284988403 secs\n",
            "\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 0.8796 Accuracy 0.5156\n",
            "Epoch 5 Batch 50 Loss 0.9422 Accuracy 0.5045\n",
            "Epoch 5 Batch 100 Loss 0.9367 Accuracy 0.5027\n",
            "Epoch 5 Batch 150 Loss 0.9361 Accuracy 0.5026\n",
            "Epoch 5 Batch 200 Loss 0.9321 Accuracy 0.5034\n",
            "Epoch 5 Batch 250 Loss 0.9280 Accuracy 0.5029\n",
            "Epoch 5 Batch 300 Loss 0.9277 Accuracy 0.5033\n",
            "Epoch 5 Batch 350 Loss 0.9218 Accuracy 0.5032\n",
            "Epoch 5 Batch 400 Loss 0.9196 Accuracy 0.5031\n",
            "Epoch 5 Batch 450 Loss 0.9182 Accuracy 0.5031\n",
            "Epoch 5 Batch 500 Loss 0.9192 Accuracy 0.5029\n",
            "Epoch 5 Batch 550 Loss 0.9185 Accuracy 0.5031\n",
            "Epoch 5 Batch 600 Loss 0.9189 Accuracy 0.5030\n",
            "Epoch 5 Batch 650 Loss 0.9186 Accuracy 0.5029\n",
            "Epoch 5 Batch 700 Loss 0.9168 Accuracy 0.5034\n",
            "Epoch 5 Batch 750 Loss 0.9154 Accuracy 0.5035\n",
            "Epoch 5 Batch 800 Loss 0.9141 Accuracy 0.5038\n",
            "Epoch 5 Batch 850 Loss 0.9130 Accuracy 0.5040\n",
            "Epoch 5 Batch 900 Loss 0.9104 Accuracy 0.5040\n",
            "Epoch 5 Batch 950 Loss 0.9090 Accuracy 0.5043\n",
            "Epoch 5 Batch 1000 Loss 0.9050 Accuracy 0.5044\n",
            "Epoch 5 Batch 1050 Loss 0.9034 Accuracy 0.5045\n",
            "Epoch 5 Batch 1100 Loss 0.9023 Accuracy 0.5047\n",
            "Epoch 5 Batch 1150 Loss 0.9005 Accuracy 0.5048\n",
            "Epoch 5 Batch 1200 Loss 0.8988 Accuracy 0.5049\n",
            "Epoch 5 Batch 1250 Loss 0.8978 Accuracy 0.5053\n",
            "Epoch 5 Batch 1300 Loss 0.8963 Accuracy 0.5060\n",
            "Epoch 5 Batch 1350 Loss 0.8938 Accuracy 0.5065\n",
            "Epoch 5 Batch 1400 Loss 0.8915 Accuracy 0.5071\n",
            "Epoch 5 Batch 1450 Loss 0.8887 Accuracy 0.5079\n",
            "Epoch 5 Batch 1500 Loss 0.8859 Accuracy 0.5087\n",
            "Epoch 5 Batch 1550 Loss 0.8839 Accuracy 0.5095\n",
            "Epoch 5 Batch 1600 Loss 0.8818 Accuracy 0.5105\n",
            "Epoch 5 Batch 1650 Loss 0.8795 Accuracy 0.5112\n",
            "Epoch 5 Batch 1700 Loss 0.8772 Accuracy 0.5121\n",
            "Epoch 5 Batch 1750 Loss 0.8749 Accuracy 0.5128\n",
            "Epoch 5 Batch 1800 Loss 0.8727 Accuracy 0.5136\n",
            "Epoch 5 Batch 1850 Loss 0.8705 Accuracy 0.5144\n",
            "Epoch 5 Batch 1900 Loss 0.8689 Accuracy 0.5152\n",
            "Epoch 5 Batch 1950 Loss 0.8666 Accuracy 0.5158\n",
            "Epoch 5 Batch 2000 Loss 0.8644 Accuracy 0.5166\n",
            "Epoch 5 Batch 2050 Loss 0.8621 Accuracy 0.5171\n",
            "Epoch 5 Batch 2100 Loss 0.8594 Accuracy 0.5175\n",
            "Epoch 5 Batch 2150 Loss 0.8569 Accuracy 0.5178\n",
            "Epoch 5 Batch 2200 Loss 0.8543 Accuracy 0.5181\n",
            "Epoch 5 Batch 2250 Loss 0.8513 Accuracy 0.5180\n",
            "Epoch 5 Batch 2300 Loss 0.8484 Accuracy 0.5184\n",
            "Epoch 5 Batch 2350 Loss 0.8458 Accuracy 0.5185\n",
            "Epoch 5 Batch 2400 Loss 0.8432 Accuracy 0.5186\n",
            "Epoch 5 Batch 2450 Loss 0.8407 Accuracy 0.5188\n",
            "Epoch 5 Batch 2500 Loss 0.8384 Accuracy 0.5190\n",
            "Epoch 5 Batch 2550 Loss 0.8358 Accuracy 0.5193\n",
            "Epoch 5 Batch 2600 Loss 0.8336 Accuracy 0.5196\n",
            "Epoch 5 Batch 2650 Loss 0.8311 Accuracy 0.5198\n",
            "Epoch 5 Batch 2700 Loss 0.8289 Accuracy 0.5201\n",
            "Epoch 5 Batch 2750 Loss 0.8264 Accuracy 0.5203\n",
            "Epoch 5 Batch 2800 Loss 0.8244 Accuracy 0.5206\n",
            "Epoch 5 Batch 2850 Loss 0.8222 Accuracy 0.5209\n",
            "Epoch 5 Batch 2900 Loss 0.8204 Accuracy 0.5212\n",
            "Epoch 5 Batch 2950 Loss 0.8187 Accuracy 0.5214\n",
            "Epoch 5 Batch 3000 Loss 0.8166 Accuracy 0.5216\n",
            "Epoch 5 Batch 3050 Loss 0.8152 Accuracy 0.5219\n",
            "Epoch 5 Batch 3100 Loss 0.8136 Accuracy 0.5221\n",
            "Epoch 5 Batch 3150 Loss 0.8118 Accuracy 0.5224\n",
            "Epoch 5 Batch 3200 Loss 0.8099 Accuracy 0.5225\n",
            "Epoch 5 Batch 3250 Loss 0.8085 Accuracy 0.5228\n",
            "Epoch 5 Batch 3300 Loss 0.8069 Accuracy 0.5231\n",
            "Epoch 5 Batch 3350 Loss 0.8053 Accuracy 0.5233\n",
            "Epoch 5 Batch 3400 Loss 0.8035 Accuracy 0.5235\n",
            "Epoch 5 Batch 3450 Loss 0.8021 Accuracy 0.5238\n",
            "Epoch 5 Batch 3500 Loss 0.8004 Accuracy 0.5240\n",
            "Epoch 5 Batch 3550 Loss 0.7988 Accuracy 0.5244\n",
            "Epoch 5 Batch 3600 Loss 0.7971 Accuracy 0.5246\n",
            "Epoch 5 Batch 3650 Loss 0.7956 Accuracy 0.5249\n",
            "Epoch 5 Batch 3700 Loss 0.7939 Accuracy 0.5252\n",
            "Epoch 5 Batch 3750 Loss 0.7923 Accuracy 0.5255\n",
            "Epoch 5 Batch 3800 Loss 0.7909 Accuracy 0.5258\n",
            "Epoch 5 Batch 3850 Loss 0.7896 Accuracy 0.5262\n",
            "Epoch 5 Batch 3900 Loss 0.7884 Accuracy 0.5265\n",
            "Epoch 5 Batch 3950 Loss 0.7871 Accuracy 0.5268\n",
            "Epoch 5 Batch 4000 Loss 0.7860 Accuracy 0.5271\n",
            "Epoch 5 Batch 4050 Loss 0.7848 Accuracy 0.5274\n",
            "Epoch 5 Batch 4100 Loss 0.7840 Accuracy 0.5276\n",
            "Epoch 5 Batch 4150 Loss 0.7838 Accuracy 0.5277\n",
            "Epoch 5 Batch 4200 Loss 0.7839 Accuracy 0.5276\n",
            "Epoch 5 Batch 4250 Loss 0.7844 Accuracy 0.5277\n",
            "Epoch 5 Batch 4300 Loss 0.7853 Accuracy 0.5275\n",
            "Epoch 5 Batch 4350 Loss 0.7864 Accuracy 0.5274\n",
            "Epoch 5 Batch 4400 Loss 0.7876 Accuracy 0.5273\n",
            "Epoch 5 Batch 4450 Loss 0.7887 Accuracy 0.5271\n",
            "Epoch 5 Batch 4500 Loss 0.7899 Accuracy 0.5269\n",
            "Epoch 5 Batch 4550 Loss 0.7912 Accuracy 0.5267\n",
            "Epoch 5 Batch 4600 Loss 0.7927 Accuracy 0.5265\n",
            "Epoch 5 Batch 4650 Loss 0.7942 Accuracy 0.5263\n",
            "Epoch 5 Batch 4700 Loss 0.7955 Accuracy 0.5261\n",
            "Epoch 5 Batch 4750 Loss 0.7969 Accuracy 0.5259\n",
            "Epoch 5 Batch 4800 Loss 0.7979 Accuracy 0.5256\n",
            "Epoch 5 Batch 4850 Loss 0.7991 Accuracy 0.5254\n",
            "Epoch 5 Batch 4900 Loss 0.8003 Accuracy 0.5253\n",
            "Epoch 5 Batch 4950 Loss 0.8017 Accuracy 0.5250\n",
            "Epoch 5 Batch 5000 Loss 0.8030 Accuracy 0.5248\n",
            "Epoch 5 Batch 5050 Loss 0.8042 Accuracy 0.5246\n",
            "Epoch 5 Batch 5100 Loss 0.8057 Accuracy 0.5244\n",
            "Epoch 5 Batch 5150 Loss 0.8069 Accuracy 0.5241\n",
            "Epoch 5 Batch 5200 Loss 0.8082 Accuracy 0.5238\n",
            "Epoch 5 Batch 5250 Loss 0.8095 Accuracy 0.5235\n",
            "Epoch 5 Batch 5300 Loss 0.8109 Accuracy 0.5232\n",
            "Epoch 5 Batch 5350 Loss 0.8121 Accuracy 0.5228\n",
            "Epoch 5 Batch 5400 Loss 0.8133 Accuracy 0.5226\n",
            "Epoch 5 Batch 5450 Loss 0.8143 Accuracy 0.5223\n",
            "Epoch 5 Batch 5500 Loss 0.8154 Accuracy 0.5220\n",
            "Epoch 5 Batch 5550 Loss 0.8165 Accuracy 0.5218\n",
            "Epoch 5 Batch 5600 Loss 0.8175 Accuracy 0.5215\n",
            "Epoch 5 Batch 5650 Loss 0.8186 Accuracy 0.5213\n",
            "Epoch 5 Batch 5700 Loss 0.8196 Accuracy 0.5210\n",
            "Saving checkpoint for epoch 5 at /content/drive/MyDrive/Projects/APM598 Project/Data/ckpt/ckpt-23\n",
            "Time taken for 1 epoch: 1587.1450583934784 secs\n",
            "\n",
            "Start of epoch 6\n",
            "Epoch 6 Batch 0 Loss 1.0177 Accuracy 0.4745\n",
            "Epoch 6 Batch 50 Loss 0.9360 Accuracy 0.4961\n",
            "Epoch 6 Batch 100 Loss 0.9201 Accuracy 0.5021\n",
            "Epoch 6 Batch 150 Loss 0.9143 Accuracy 0.5023\n",
            "Epoch 6 Batch 200 Loss 0.9135 Accuracy 0.5016\n",
            "Epoch 6 Batch 250 Loss 0.9167 Accuracy 0.5021\n",
            "Epoch 6 Batch 300 Loss 0.9153 Accuracy 0.5018\n",
            "Epoch 6 Batch 350 Loss 0.9147 Accuracy 0.5025\n",
            "Epoch 6 Batch 400 Loss 0.9128 Accuracy 0.5026\n",
            "Epoch 6 Batch 450 Loss 0.9105 Accuracy 0.5027\n",
            "Epoch 6 Batch 500 Loss 0.9103 Accuracy 0.5026\n",
            "Epoch 6 Batch 550 Loss 0.9096 Accuracy 0.5027\n",
            "Epoch 6 Batch 600 Loss 0.9098 Accuracy 0.5029\n",
            "Epoch 6 Batch 650 Loss 0.9100 Accuracy 0.5031\n",
            "Epoch 6 Batch 700 Loss 0.9086 Accuracy 0.5036\n",
            "Epoch 6 Batch 750 Loss 0.9081 Accuracy 0.5040\n",
            "Epoch 6 Batch 800 Loss 0.9069 Accuracy 0.5043\n",
            "Epoch 6 Batch 850 Loss 0.9059 Accuracy 0.5049\n",
            "Epoch 6 Batch 900 Loss 0.9043 Accuracy 0.5048\n",
            "Epoch 6 Batch 950 Loss 0.9022 Accuracy 0.5048\n",
            "Epoch 6 Batch 1000 Loss 0.8993 Accuracy 0.5048\n",
            "Epoch 6 Batch 1050 Loss 0.8996 Accuracy 0.5051\n",
            "Epoch 6 Batch 1100 Loss 0.8986 Accuracy 0.5052\n",
            "Epoch 6 Batch 1150 Loss 0.8978 Accuracy 0.5055\n",
            "Epoch 6 Batch 1200 Loss 0.8956 Accuracy 0.5057\n",
            "Epoch 6 Batch 1250 Loss 0.8942 Accuracy 0.5061\n",
            "Epoch 6 Batch 1300 Loss 0.8923 Accuracy 0.5065\n",
            "Epoch 6 Batch 1350 Loss 0.8896 Accuracy 0.5071\n",
            "Epoch 6 Batch 1400 Loss 0.8874 Accuracy 0.5077\n",
            "Epoch 6 Batch 1450 Loss 0.8857 Accuracy 0.5085\n",
            "Epoch 6 Batch 1500 Loss 0.8834 Accuracy 0.5093\n",
            "Epoch 6 Batch 1550 Loss 0.8810 Accuracy 0.5101\n",
            "Epoch 6 Batch 1600 Loss 0.8784 Accuracy 0.5110\n",
            "Epoch 6 Batch 1650 Loss 0.8766 Accuracy 0.5118\n",
            "Epoch 6 Batch 1700 Loss 0.8743 Accuracy 0.5125\n",
            "Epoch 6 Batch 1750 Loss 0.8716 Accuracy 0.5132\n",
            "Epoch 6 Batch 1800 Loss 0.8696 Accuracy 0.5139\n",
            "Epoch 6 Batch 1850 Loss 0.8671 Accuracy 0.5147\n",
            "Epoch 6 Batch 1900 Loss 0.8649 Accuracy 0.5155\n",
            "Epoch 6 Batch 1950 Loss 0.8632 Accuracy 0.5162\n",
            "Epoch 6 Batch 2000 Loss 0.8613 Accuracy 0.5169\n",
            "Epoch 6 Batch 2050 Loss 0.8597 Accuracy 0.5174\n",
            "Epoch 6 Batch 2100 Loss 0.8573 Accuracy 0.5177\n",
            "Epoch 6 Batch 2150 Loss 0.8545 Accuracy 0.5180\n",
            "Epoch 6 Batch 2200 Loss 0.8518 Accuracy 0.5182\n",
            "Epoch 6 Batch 2250 Loss 0.8486 Accuracy 0.5183\n",
            "Epoch 6 Batch 2300 Loss 0.8455 Accuracy 0.5186\n",
            "Epoch 6 Batch 2350 Loss 0.8426 Accuracy 0.5190\n",
            "Epoch 6 Batch 2400 Loss 0.8400 Accuracy 0.5192\n",
            "Epoch 6 Batch 2450 Loss 0.8369 Accuracy 0.5195\n",
            "Epoch 6 Batch 2500 Loss 0.8345 Accuracy 0.5198\n",
            "Epoch 6 Batch 2550 Loss 0.8318 Accuracy 0.5202\n",
            "Epoch 6 Batch 2600 Loss 0.8296 Accuracy 0.5204\n",
            "Epoch 6 Batch 2650 Loss 0.8269 Accuracy 0.5207\n",
            "Epoch 6 Batch 2700 Loss 0.8246 Accuracy 0.5211\n",
            "Epoch 6 Batch 2750 Loss 0.8223 Accuracy 0.5213\n",
            "Epoch 6 Batch 2800 Loss 0.8202 Accuracy 0.5215\n",
            "Epoch 6 Batch 2850 Loss 0.8185 Accuracy 0.5217\n",
            "Epoch 6 Batch 2900 Loss 0.8168 Accuracy 0.5219\n",
            "Epoch 6 Batch 2950 Loss 0.8152 Accuracy 0.5222\n",
            "Epoch 6 Batch 3000 Loss 0.8133 Accuracy 0.5224\n",
            "Epoch 6 Batch 3050 Loss 0.8117 Accuracy 0.5226\n",
            "Epoch 6 Batch 3100 Loss 0.8099 Accuracy 0.5228\n",
            "Epoch 6 Batch 3150 Loss 0.8080 Accuracy 0.5229\n",
            "Epoch 6 Batch 3200 Loss 0.8064 Accuracy 0.5232\n",
            "Epoch 6 Batch 3250 Loss 0.8046 Accuracy 0.5234\n",
            "Epoch 6 Batch 3300 Loss 0.8028 Accuracy 0.5236\n",
            "Epoch 6 Batch 3350 Loss 0.8010 Accuracy 0.5240\n",
            "Epoch 6 Batch 3400 Loss 0.7995 Accuracy 0.5242\n",
            "Epoch 6 Batch 3450 Loss 0.7977 Accuracy 0.5244\n",
            "Epoch 6 Batch 3500 Loss 0.7959 Accuracy 0.5247\n",
            "Epoch 6 Batch 3550 Loss 0.7944 Accuracy 0.5250\n",
            "Epoch 6 Batch 3600 Loss 0.7928 Accuracy 0.5253\n",
            "Epoch 6 Batch 3650 Loss 0.7913 Accuracy 0.5256\n",
            "Epoch 6 Batch 3700 Loss 0.7899 Accuracy 0.5258\n",
            "Epoch 6 Batch 3750 Loss 0.7885 Accuracy 0.5261\n",
            "Epoch 6 Batch 3800 Loss 0.7871 Accuracy 0.5264\n",
            "Epoch 6 Batch 3850 Loss 0.7859 Accuracy 0.5267\n",
            "Epoch 6 Batch 3900 Loss 0.7848 Accuracy 0.5270\n",
            "Epoch 6 Batch 3950 Loss 0.7837 Accuracy 0.5273\n",
            "Epoch 6 Batch 4000 Loss 0.7824 Accuracy 0.5276\n",
            "Epoch 6 Batch 4050 Loss 0.7812 Accuracy 0.5279\n",
            "Epoch 6 Batch 4100 Loss 0.7801 Accuracy 0.5280\n",
            "Epoch 6 Batch 4150 Loss 0.7796 Accuracy 0.5281\n",
            "Epoch 6 Batch 4200 Loss 0.7800 Accuracy 0.5281\n",
            "Epoch 6 Batch 4250 Loss 0.7807 Accuracy 0.5281\n",
            "Epoch 6 Batch 4300 Loss 0.7815 Accuracy 0.5280\n",
            "Epoch 6 Batch 4350 Loss 0.7825 Accuracy 0.5278\n",
            "Epoch 6 Batch 4400 Loss 0.7834 Accuracy 0.5277\n",
            "Epoch 6 Batch 4450 Loss 0.7848 Accuracy 0.5275\n",
            "Epoch 6 Batch 4500 Loss 0.7861 Accuracy 0.5274\n",
            "Epoch 6 Batch 4550 Loss 0.7876 Accuracy 0.5272\n",
            "Epoch 6 Batch 4600 Loss 0.7891 Accuracy 0.5270\n",
            "Epoch 6 Batch 4650 Loss 0.7905 Accuracy 0.5268\n",
            "Epoch 6 Batch 4700 Loss 0.7919 Accuracy 0.5266\n",
            "Epoch 6 Batch 4750 Loss 0.7934 Accuracy 0.5264\n",
            "Epoch 6 Batch 4800 Loss 0.7947 Accuracy 0.5262\n",
            "Epoch 6 Batch 4850 Loss 0.7958 Accuracy 0.5260\n",
            "Epoch 6 Batch 4900 Loss 0.7967 Accuracy 0.5258\n",
            "Epoch 6 Batch 4950 Loss 0.7979 Accuracy 0.5256\n",
            "Epoch 6 Batch 5000 Loss 0.7994 Accuracy 0.5254\n",
            "Epoch 6 Batch 5050 Loss 0.8007 Accuracy 0.5252\n",
            "Epoch 6 Batch 5100 Loss 0.8021 Accuracy 0.5250\n",
            "Epoch 6 Batch 5150 Loss 0.8035 Accuracy 0.5247\n",
            "Epoch 6 Batch 5200 Loss 0.8048 Accuracy 0.5244\n",
            "Epoch 6 Batch 5250 Loss 0.8061 Accuracy 0.5241\n",
            "Epoch 6 Batch 5300 Loss 0.8072 Accuracy 0.5238\n",
            "Epoch 6 Batch 5350 Loss 0.8084 Accuracy 0.5235\n",
            "Epoch 6 Batch 5400 Loss 0.8097 Accuracy 0.5232\n",
            "Epoch 6 Batch 5450 Loss 0.8108 Accuracy 0.5229\n",
            "Epoch 6 Batch 5500 Loss 0.8118 Accuracy 0.5226\n",
            "Epoch 6 Batch 5550 Loss 0.8128 Accuracy 0.5224\n",
            "Epoch 6 Batch 5600 Loss 0.8138 Accuracy 0.5221\n",
            "Epoch 6 Batch 5650 Loss 0.8148 Accuracy 0.5219\n",
            "Epoch 6 Batch 5700 Loss 0.8157 Accuracy 0.5217\n",
            "Saving checkpoint for epoch 6 at /content/drive/MyDrive/Projects/APM598 Project/Data/ckpt/ckpt-24\n",
            "Time taken for 1 epoch: 1574.5608208179474 secs\n",
            "\n",
            "Start of epoch 7\n",
            "Epoch 7 Batch 0 Loss 1.0631 Accuracy 0.4918\n",
            "Epoch 7 Batch 50 Loss 0.9139 Accuracy 0.4991\n",
            "Epoch 7 Batch 100 Loss 0.9255 Accuracy 0.5004\n",
            "Epoch 7 Batch 150 Loss 0.9134 Accuracy 0.5027\n",
            "Epoch 7 Batch 200 Loss 0.9132 Accuracy 0.5039\n",
            "Epoch 7 Batch 250 Loss 0.9119 Accuracy 0.5041\n",
            "Epoch 7 Batch 300 Loss 0.9125 Accuracy 0.5046\n",
            "Epoch 7 Batch 350 Loss 0.9148 Accuracy 0.5047\n",
            "Epoch 7 Batch 400 Loss 0.9114 Accuracy 0.5048\n",
            "Epoch 7 Batch 450 Loss 0.9093 Accuracy 0.5046\n",
            "Epoch 7 Batch 500 Loss 0.9074 Accuracy 0.5038\n",
            "Epoch 7 Batch 550 Loss 0.9067 Accuracy 0.5037\n",
            "Epoch 7 Batch 600 Loss 0.9042 Accuracy 0.5042\n",
            "Epoch 7 Batch 650 Loss 0.9026 Accuracy 0.5046\n",
            "Epoch 7 Batch 700 Loss 0.9042 Accuracy 0.5052\n",
            "Epoch 7 Batch 750 Loss 0.9039 Accuracy 0.5054\n",
            "Epoch 7 Batch 800 Loss 0.9041 Accuracy 0.5055\n",
            "Epoch 7 Batch 850 Loss 0.9034 Accuracy 0.5056\n",
            "Epoch 7 Batch 900 Loss 0.9015 Accuracy 0.5057\n",
            "Epoch 7 Batch 950 Loss 0.8988 Accuracy 0.5055\n",
            "Epoch 7 Batch 1000 Loss 0.8967 Accuracy 0.5054\n",
            "Epoch 7 Batch 1050 Loss 0.8960 Accuracy 0.5055\n",
            "Epoch 7 Batch 1100 Loss 0.8959 Accuracy 0.5056\n",
            "Epoch 7 Batch 1150 Loss 0.8954 Accuracy 0.5057\n",
            "Epoch 7 Batch 1200 Loss 0.8944 Accuracy 0.5060\n",
            "Epoch 7 Batch 1250 Loss 0.8917 Accuracy 0.5066\n",
            "Epoch 7 Batch 1300 Loss 0.8888 Accuracy 0.5070\n",
            "Epoch 7 Batch 1350 Loss 0.8864 Accuracy 0.5076\n",
            "Epoch 7 Batch 1400 Loss 0.8844 Accuracy 0.5083\n",
            "Epoch 7 Batch 1450 Loss 0.8818 Accuracy 0.5092\n",
            "Epoch 7 Batch 1500 Loss 0.8790 Accuracy 0.5101\n",
            "Epoch 7 Batch 1550 Loss 0.8766 Accuracy 0.5107\n",
            "Epoch 7 Batch 1600 Loss 0.8744 Accuracy 0.5117\n",
            "Epoch 7 Batch 1650 Loss 0.8721 Accuracy 0.5126\n",
            "Epoch 7 Batch 1700 Loss 0.8693 Accuracy 0.5136\n",
            "Epoch 7 Batch 1750 Loss 0.8673 Accuracy 0.5145\n",
            "Epoch 7 Batch 1800 Loss 0.8653 Accuracy 0.5153\n",
            "Epoch 7 Batch 1850 Loss 0.8626 Accuracy 0.5160\n",
            "Epoch 7 Batch 1900 Loss 0.8609 Accuracy 0.5166\n",
            "Epoch 7 Batch 1950 Loss 0.8588 Accuracy 0.5172\n",
            "Epoch 7 Batch 2000 Loss 0.8564 Accuracy 0.5179\n",
            "Epoch 7 Batch 2050 Loss 0.8543 Accuracy 0.5184\n",
            "Epoch 7 Batch 2100 Loss 0.8523 Accuracy 0.5187\n",
            "Epoch 7 Batch 2150 Loss 0.8496 Accuracy 0.5190\n",
            "Epoch 7 Batch 2200 Loss 0.8469 Accuracy 0.5192\n",
            "Epoch 7 Batch 2250 Loss 0.8441 Accuracy 0.5193\n",
            "Epoch 7 Batch 2300 Loss 0.8415 Accuracy 0.5194\n",
            "Epoch 7 Batch 2350 Loss 0.8382 Accuracy 0.5198\n",
            "Epoch 7 Batch 2400 Loss 0.8355 Accuracy 0.5201\n",
            "Epoch 7 Batch 2450 Loss 0.8331 Accuracy 0.5202\n",
            "Epoch 7 Batch 2500 Loss 0.8304 Accuracy 0.5204\n",
            "Epoch 7 Batch 2550 Loss 0.8280 Accuracy 0.5208\n",
            "Epoch 7 Batch 2600 Loss 0.8256 Accuracy 0.5211\n",
            "Epoch 7 Batch 2650 Loss 0.8232 Accuracy 0.5213\n",
            "Epoch 7 Batch 2700 Loss 0.8211 Accuracy 0.5216\n",
            "Epoch 7 Batch 2750 Loss 0.8195 Accuracy 0.5218\n",
            "Epoch 7 Batch 2800 Loss 0.8170 Accuracy 0.5222\n",
            "Epoch 7 Batch 2850 Loss 0.8153 Accuracy 0.5226\n",
            "Epoch 7 Batch 2900 Loss 0.8133 Accuracy 0.5228\n",
            "Epoch 7 Batch 2950 Loss 0.8114 Accuracy 0.5231\n",
            "Epoch 7 Batch 3000 Loss 0.8098 Accuracy 0.5232\n",
            "Epoch 7 Batch 3050 Loss 0.8081 Accuracy 0.5234\n",
            "Epoch 7 Batch 3100 Loss 0.8062 Accuracy 0.5236\n",
            "Epoch 7 Batch 3150 Loss 0.8045 Accuracy 0.5238\n",
            "Epoch 7 Batch 3200 Loss 0.8027 Accuracy 0.5240\n",
            "Epoch 7 Batch 3250 Loss 0.8012 Accuracy 0.5242\n",
            "Epoch 7 Batch 3300 Loss 0.7995 Accuracy 0.5244\n",
            "Epoch 7 Batch 3350 Loss 0.7976 Accuracy 0.5246\n",
            "Epoch 7 Batch 3400 Loss 0.7959 Accuracy 0.5249\n",
            "Epoch 7 Batch 3450 Loss 0.7941 Accuracy 0.5252\n",
            "Epoch 7 Batch 3500 Loss 0.7923 Accuracy 0.5255\n",
            "Epoch 7 Batch 3550 Loss 0.7905 Accuracy 0.5259\n",
            "Epoch 7 Batch 3600 Loss 0.7889 Accuracy 0.5262\n",
            "Epoch 7 Batch 3650 Loss 0.7869 Accuracy 0.5265\n",
            "Epoch 7 Batch 3700 Loss 0.7857 Accuracy 0.5268\n",
            "Epoch 7 Batch 3750 Loss 0.7843 Accuracy 0.5271\n",
            "Epoch 7 Batch 3800 Loss 0.7831 Accuracy 0.5275\n",
            "Epoch 7 Batch 3850 Loss 0.7817 Accuracy 0.5278\n",
            "Epoch 7 Batch 3900 Loss 0.7804 Accuracy 0.5281\n",
            "Epoch 7 Batch 3950 Loss 0.7794 Accuracy 0.5284\n",
            "Epoch 7 Batch 4000 Loss 0.7782 Accuracy 0.5286\n",
            "Epoch 7 Batch 4050 Loss 0.7773 Accuracy 0.5289\n",
            "Epoch 7 Batch 4100 Loss 0.7766 Accuracy 0.5291\n",
            "Epoch 7 Batch 4150 Loss 0.7761 Accuracy 0.5291\n",
            "Epoch 7 Batch 4200 Loss 0.7762 Accuracy 0.5291\n",
            "Epoch 7 Batch 4250 Loss 0.7767 Accuracy 0.5290\n",
            "Epoch 7 Batch 4300 Loss 0.7775 Accuracy 0.5289\n",
            "Epoch 7 Batch 4350 Loss 0.7784 Accuracy 0.5288\n",
            "Epoch 7 Batch 4400 Loss 0.7795 Accuracy 0.5287\n",
            "Epoch 7 Batch 4450 Loss 0.7810 Accuracy 0.5284\n",
            "Epoch 7 Batch 4500 Loss 0.7824 Accuracy 0.5282\n",
            "Epoch 7 Batch 4550 Loss 0.7836 Accuracy 0.5280\n",
            "Epoch 7 Batch 4600 Loss 0.7850 Accuracy 0.5278\n",
            "Epoch 7 Batch 4650 Loss 0.7863 Accuracy 0.5277\n",
            "Epoch 7 Batch 4700 Loss 0.7876 Accuracy 0.5274\n",
            "Epoch 7 Batch 4750 Loss 0.7890 Accuracy 0.5272\n",
            "Epoch 7 Batch 4800 Loss 0.7902 Accuracy 0.5270\n",
            "Epoch 7 Batch 4850 Loss 0.7915 Accuracy 0.5268\n",
            "Epoch 7 Batch 4900 Loss 0.7927 Accuracy 0.5266\n",
            "Epoch 7 Batch 4950 Loss 0.7944 Accuracy 0.5264\n",
            "Epoch 7 Batch 5000 Loss 0.7956 Accuracy 0.5262\n",
            "Epoch 7 Batch 5050 Loss 0.7968 Accuracy 0.5260\n",
            "Epoch 7 Batch 5100 Loss 0.7983 Accuracy 0.5258\n",
            "Epoch 7 Batch 5150 Loss 0.7995 Accuracy 0.5255\n",
            "Epoch 7 Batch 5200 Loss 0.8008 Accuracy 0.5252\n",
            "Epoch 7 Batch 5250 Loss 0.8022 Accuracy 0.5249\n",
            "Epoch 7 Batch 5300 Loss 0.8032 Accuracy 0.5246\n",
            "Epoch 7 Batch 5350 Loss 0.8045 Accuracy 0.5243\n",
            "Epoch 7 Batch 5400 Loss 0.8056 Accuracy 0.5240\n",
            "Epoch 7 Batch 5450 Loss 0.8069 Accuracy 0.5237\n",
            "Epoch 7 Batch 5500 Loss 0.8079 Accuracy 0.5235\n",
            "Epoch 7 Batch 5550 Loss 0.8088 Accuracy 0.5232\n",
            "Epoch 7 Batch 5600 Loss 0.8098 Accuracy 0.5229\n",
            "Epoch 7 Batch 5650 Loss 0.8108 Accuracy 0.5227\n",
            "Epoch 7 Batch 5700 Loss 0.8118 Accuracy 0.5224\n",
            "Saving checkpoint for epoch 7 at /content/drive/MyDrive/Projects/APM598 Project/Data/ckpt/ckpt-25\n",
            "Time taken for 1 epoch: 1568.2347149848938 secs\n",
            "\n",
            "Start of epoch 8\n",
            "Epoch 8 Batch 0 Loss 0.8892 Accuracy 0.4572\n",
            "Epoch 8 Batch 50 Loss 0.9402 Accuracy 0.5044\n",
            "Epoch 8 Batch 100 Loss 0.9308 Accuracy 0.5055\n",
            "Epoch 8 Batch 150 Loss 0.9165 Accuracy 0.5063\n",
            "Epoch 8 Batch 200 Loss 0.9142 Accuracy 0.5056\n",
            "Epoch 8 Batch 250 Loss 0.9146 Accuracy 0.5060\n",
            "Epoch 8 Batch 300 Loss 0.9143 Accuracy 0.5054\n",
            "Epoch 8 Batch 350 Loss 0.9135 Accuracy 0.5056\n",
            "Epoch 8 Batch 400 Loss 0.9088 Accuracy 0.5045\n",
            "Epoch 8 Batch 450 Loss 0.9058 Accuracy 0.5050\n",
            "Epoch 8 Batch 500 Loss 0.9035 Accuracy 0.5045\n",
            "Epoch 8 Batch 550 Loss 0.9012 Accuracy 0.5048\n",
            "Epoch 8 Batch 600 Loss 0.8990 Accuracy 0.5050\n",
            "Epoch 8 Batch 650 Loss 0.8989 Accuracy 0.5054\n",
            "Epoch 8 Batch 700 Loss 0.8996 Accuracy 0.5055\n",
            "Epoch 8 Batch 750 Loss 0.8995 Accuracy 0.5057\n",
            "Epoch 8 Batch 800 Loss 0.8977 Accuracy 0.5056\n",
            "Epoch 8 Batch 850 Loss 0.8962 Accuracy 0.5060\n",
            "Epoch 8 Batch 900 Loss 0.8955 Accuracy 0.5060\n",
            "Epoch 8 Batch 950 Loss 0.8933 Accuracy 0.5061\n",
            "Epoch 8 Batch 1000 Loss 0.8918 Accuracy 0.5062\n",
            "Epoch 8 Batch 1050 Loss 0.8912 Accuracy 0.5065\n",
            "Epoch 8 Batch 1100 Loss 0.8903 Accuracy 0.5067\n",
            "Epoch 8 Batch 1150 Loss 0.8902 Accuracy 0.5066\n",
            "Epoch 8 Batch 1200 Loss 0.8888 Accuracy 0.5068\n",
            "Epoch 8 Batch 1250 Loss 0.8868 Accuracy 0.5071\n",
            "Epoch 8 Batch 1300 Loss 0.8849 Accuracy 0.5074\n",
            "Epoch 8 Batch 1350 Loss 0.8831 Accuracy 0.5081\n",
            "Epoch 8 Batch 1400 Loss 0.8806 Accuracy 0.5087\n",
            "Epoch 8 Batch 1450 Loss 0.8783 Accuracy 0.5095\n",
            "Epoch 8 Batch 1500 Loss 0.8758 Accuracy 0.5101\n",
            "Epoch 8 Batch 1550 Loss 0.8738 Accuracy 0.5110\n",
            "Epoch 8 Batch 1600 Loss 0.8704 Accuracy 0.5118\n",
            "Epoch 8 Batch 1650 Loss 0.8683 Accuracy 0.5127\n",
            "Epoch 8 Batch 1700 Loss 0.8657 Accuracy 0.5136\n",
            "Epoch 8 Batch 1750 Loss 0.8639 Accuracy 0.5144\n",
            "Epoch 8 Batch 1800 Loss 0.8615 Accuracy 0.5152\n",
            "Epoch 8 Batch 1850 Loss 0.8590 Accuracy 0.5161\n",
            "Epoch 8 Batch 1900 Loss 0.8566 Accuracy 0.5170\n",
            "Epoch 8 Batch 1950 Loss 0.8547 Accuracy 0.5178\n",
            "Epoch 8 Batch 2000 Loss 0.8526 Accuracy 0.5185\n",
            "Epoch 8 Batch 2050 Loss 0.8506 Accuracy 0.5189\n",
            "Epoch 8 Batch 2100 Loss 0.8484 Accuracy 0.5192\n",
            "Epoch 8 Batch 2150 Loss 0.8456 Accuracy 0.5194\n",
            "Epoch 8 Batch 2200 Loss 0.8429 Accuracy 0.5196\n",
            "Epoch 8 Batch 2250 Loss 0.8402 Accuracy 0.5197\n",
            "Epoch 8 Batch 2300 Loss 0.8377 Accuracy 0.5200\n",
            "Epoch 8 Batch 2350 Loss 0.8351 Accuracy 0.5201\n",
            "Epoch 8 Batch 2400 Loss 0.8326 Accuracy 0.5204\n",
            "Epoch 8 Batch 2450 Loss 0.8299 Accuracy 0.5207\n",
            "Epoch 8 Batch 2500 Loss 0.8274 Accuracy 0.5211\n",
            "Epoch 8 Batch 2550 Loss 0.8246 Accuracy 0.5215\n",
            "Epoch 8 Batch 2600 Loss 0.8223 Accuracy 0.5217\n",
            "Epoch 8 Batch 2650 Loss 0.8199 Accuracy 0.5219\n",
            "Epoch 8 Batch 2700 Loss 0.8175 Accuracy 0.5220\n",
            "Epoch 8 Batch 2750 Loss 0.8155 Accuracy 0.5223\n",
            "Epoch 8 Batch 2800 Loss 0.8135 Accuracy 0.5226\n",
            "Epoch 8 Batch 2850 Loss 0.8113 Accuracy 0.5229\n",
            "Epoch 8 Batch 2900 Loss 0.8095 Accuracy 0.5232\n",
            "Epoch 8 Batch 2950 Loss 0.8076 Accuracy 0.5234\n",
            "Epoch 8 Batch 3000 Loss 0.8059 Accuracy 0.5235\n",
            "Epoch 8 Batch 3050 Loss 0.8042 Accuracy 0.5238\n",
            "Epoch 8 Batch 3100 Loss 0.8025 Accuracy 0.5241\n",
            "Epoch 8 Batch 3150 Loss 0.8009 Accuracy 0.5243\n",
            "Epoch 8 Batch 3200 Loss 0.7992 Accuracy 0.5244\n",
            "Epoch 8 Batch 3250 Loss 0.7974 Accuracy 0.5247\n",
            "Epoch 8 Batch 3300 Loss 0.7955 Accuracy 0.5249\n",
            "Epoch 8 Batch 3350 Loss 0.7938 Accuracy 0.5251\n",
            "Epoch 8 Batch 3400 Loss 0.7922 Accuracy 0.5253\n",
            "Epoch 8 Batch 3450 Loss 0.7906 Accuracy 0.5256\n",
            "Epoch 8 Batch 3500 Loss 0.7890 Accuracy 0.5259\n",
            "Epoch 8 Batch 3550 Loss 0.7874 Accuracy 0.5263\n",
            "Epoch 8 Batch 3600 Loss 0.7857 Accuracy 0.5266\n",
            "Epoch 8 Batch 3650 Loss 0.7843 Accuracy 0.5269\n",
            "Epoch 8 Batch 3700 Loss 0.7827 Accuracy 0.5272\n",
            "Epoch 8 Batch 3750 Loss 0.7815 Accuracy 0.5275\n",
            "Epoch 8 Batch 3800 Loss 0.7800 Accuracy 0.5278\n",
            "Epoch 8 Batch 3850 Loss 0.7786 Accuracy 0.5281\n",
            "Epoch 8 Batch 3900 Loss 0.7776 Accuracy 0.5283\n",
            "Epoch 8 Batch 3950 Loss 0.7763 Accuracy 0.5286\n",
            "Epoch 8 Batch 4000 Loss 0.7750 Accuracy 0.5289\n",
            "Epoch 8 Batch 4050 Loss 0.7741 Accuracy 0.5292\n",
            "Epoch 8 Batch 4100 Loss 0.7731 Accuracy 0.5294\n",
            "Epoch 8 Batch 4150 Loss 0.7728 Accuracy 0.5295\n",
            "Epoch 8 Batch 4200 Loss 0.7732 Accuracy 0.5295\n",
            "Epoch 8 Batch 4250 Loss 0.7736 Accuracy 0.5295\n",
            "Epoch 8 Batch 4300 Loss 0.7743 Accuracy 0.5294\n",
            "Epoch 8 Batch 4350 Loss 0.7753 Accuracy 0.5293\n",
            "Epoch 8 Batch 4400 Loss 0.7763 Accuracy 0.5291\n",
            "Epoch 8 Batch 4450 Loss 0.7774 Accuracy 0.5289\n",
            "Epoch 8 Batch 4500 Loss 0.7788 Accuracy 0.5287\n",
            "Epoch 8 Batch 4550 Loss 0.7802 Accuracy 0.5285\n",
            "Epoch 8 Batch 4600 Loss 0.7815 Accuracy 0.5283\n",
            "Epoch 8 Batch 4650 Loss 0.7829 Accuracy 0.5280\n",
            "Epoch 8 Batch 4700 Loss 0.7842 Accuracy 0.5278\n",
            "Epoch 8 Batch 4750 Loss 0.7856 Accuracy 0.5277\n",
            "Epoch 8 Batch 4800 Loss 0.7869 Accuracy 0.5275\n",
            "Epoch 8 Batch 4850 Loss 0.7878 Accuracy 0.5274\n",
            "Epoch 8 Batch 4900 Loss 0.7890 Accuracy 0.5271\n",
            "Epoch 8 Batch 4950 Loss 0.7905 Accuracy 0.5269\n",
            "Epoch 8 Batch 5000 Loss 0.7920 Accuracy 0.5266\n",
            "Epoch 8 Batch 5050 Loss 0.7933 Accuracy 0.5264\n",
            "Epoch 8 Batch 5100 Loss 0.7945 Accuracy 0.5262\n",
            "Epoch 8 Batch 5150 Loss 0.7958 Accuracy 0.5259\n",
            "Epoch 8 Batch 5200 Loss 0.7971 Accuracy 0.5257\n",
            "Epoch 8 Batch 5250 Loss 0.7983 Accuracy 0.5254\n",
            "Epoch 8 Batch 5300 Loss 0.7996 Accuracy 0.5250\n",
            "Epoch 8 Batch 5350 Loss 0.8008 Accuracy 0.5247\n",
            "Epoch 8 Batch 5400 Loss 0.8020 Accuracy 0.5244\n",
            "Epoch 8 Batch 5450 Loss 0.8030 Accuracy 0.5241\n",
            "Epoch 8 Batch 5500 Loss 0.8041 Accuracy 0.5238\n",
            "Epoch 8 Batch 5550 Loss 0.8053 Accuracy 0.5236\n",
            "Epoch 8 Batch 5600 Loss 0.8065 Accuracy 0.5234\n",
            "Epoch 8 Batch 5650 Loss 0.8074 Accuracy 0.5231\n",
            "Epoch 8 Batch 5700 Loss 0.8083 Accuracy 0.5229\n",
            "Saving checkpoint for epoch 8 at /content/drive/MyDrive/Projects/APM598 Project/Data/ckpt/ckpt-26\n",
            "Time taken for 1 epoch: 1577.0678217411041 secs\n",
            "\n",
            "Start of epoch 9\n",
            "Epoch 9 Batch 0 Loss 0.8213 Accuracy 0.5148\n",
            "Epoch 9 Batch 50 Loss 0.9012 Accuracy 0.5082\n",
            "Epoch 9 Batch 100 Loss 0.9042 Accuracy 0.5082\n",
            "Epoch 9 Batch 150 Loss 0.9045 Accuracy 0.5086\n",
            "Epoch 9 Batch 200 Loss 0.9012 Accuracy 0.5093\n",
            "Epoch 9 Batch 250 Loss 0.9003 Accuracy 0.5077\n",
            "Epoch 9 Batch 300 Loss 0.9023 Accuracy 0.5072\n",
            "Epoch 9 Batch 350 Loss 0.9015 Accuracy 0.5068\n",
            "Epoch 9 Batch 400 Loss 0.8989 Accuracy 0.5069\n",
            "Epoch 9 Batch 450 Loss 0.9001 Accuracy 0.5066\n",
            "Epoch 9 Batch 500 Loss 0.8990 Accuracy 0.5063\n",
            "Epoch 9 Batch 550 Loss 0.8988 Accuracy 0.5062\n",
            "Epoch 9 Batch 600 Loss 0.8979 Accuracy 0.5062\n",
            "Epoch 9 Batch 650 Loss 0.8965 Accuracy 0.5066\n",
            "Epoch 9 Batch 700 Loss 0.8961 Accuracy 0.5068\n",
            "Epoch 9 Batch 750 Loss 0.8965 Accuracy 0.5071\n",
            "Epoch 9 Batch 800 Loss 0.8972 Accuracy 0.5072\n",
            "Epoch 9 Batch 850 Loss 0.8952 Accuracy 0.5071\n",
            "Epoch 9 Batch 900 Loss 0.8942 Accuracy 0.5070\n",
            "Epoch 9 Batch 950 Loss 0.8922 Accuracy 0.5073\n",
            "Epoch 9 Batch 1000 Loss 0.8888 Accuracy 0.5072\n",
            "Epoch 9 Batch 1050 Loss 0.8886 Accuracy 0.5074\n",
            "Epoch 9 Batch 1100 Loss 0.8875 Accuracy 0.5076\n",
            "Epoch 9 Batch 1150 Loss 0.8856 Accuracy 0.5078\n",
            "Epoch 9 Batch 1200 Loss 0.8843 Accuracy 0.5080\n",
            "Epoch 9 Batch 1250 Loss 0.8824 Accuracy 0.5084\n",
            "Epoch 9 Batch 1300 Loss 0.8805 Accuracy 0.5088\n",
            "Epoch 9 Batch 1350 Loss 0.8781 Accuracy 0.5095\n",
            "Epoch 9 Batch 1400 Loss 0.8759 Accuracy 0.5102\n",
            "Epoch 9 Batch 1450 Loss 0.8740 Accuracy 0.5108\n",
            "Epoch 9 Batch 1500 Loss 0.8720 Accuracy 0.5116\n",
            "Epoch 9 Batch 1550 Loss 0.8693 Accuracy 0.5125\n",
            "Epoch 9 Batch 1600 Loss 0.8671 Accuracy 0.5132\n",
            "Epoch 9 Batch 1650 Loss 0.8645 Accuracy 0.5142\n",
            "Epoch 9 Batch 1700 Loss 0.8625 Accuracy 0.5149\n",
            "Epoch 9 Batch 1750 Loss 0.8606 Accuracy 0.5157\n",
            "Epoch 9 Batch 1800 Loss 0.8583 Accuracy 0.5164\n",
            "Epoch 9 Batch 1850 Loss 0.8559 Accuracy 0.5171\n",
            "Epoch 9 Batch 1900 Loss 0.8540 Accuracy 0.5179\n",
            "Epoch 9 Batch 1950 Loss 0.8516 Accuracy 0.5186\n",
            "Epoch 9 Batch 2000 Loss 0.8489 Accuracy 0.5191\n",
            "Epoch 9 Batch 2050 Loss 0.8473 Accuracy 0.5197\n",
            "Epoch 9 Batch 2100 Loss 0.8449 Accuracy 0.5201\n",
            "Epoch 9 Batch 2150 Loss 0.8422 Accuracy 0.5204\n",
            "Epoch 9 Batch 2200 Loss 0.8393 Accuracy 0.5206\n",
            "Epoch 9 Batch 2250 Loss 0.8364 Accuracy 0.5207\n",
            "Epoch 9 Batch 2300 Loss 0.8335 Accuracy 0.5207\n",
            "Epoch 9 Batch 2350 Loss 0.8309 Accuracy 0.5210\n",
            "Epoch 9 Batch 2400 Loss 0.8285 Accuracy 0.5213\n",
            "Epoch 9 Batch 2450 Loss 0.8260 Accuracy 0.5216\n",
            "Epoch 9 Batch 2500 Loss 0.8233 Accuracy 0.5217\n",
            "Epoch 9 Batch 2550 Loss 0.8207 Accuracy 0.5220\n",
            "Epoch 9 Batch 2600 Loss 0.8181 Accuracy 0.5224\n",
            "Epoch 9 Batch 2650 Loss 0.8155 Accuracy 0.5227\n",
            "Epoch 9 Batch 2700 Loss 0.8134 Accuracy 0.5229\n",
            "Epoch 9 Batch 2750 Loss 0.8113 Accuracy 0.5231\n",
            "Epoch 9 Batch 2800 Loss 0.8094 Accuracy 0.5233\n",
            "Epoch 9 Batch 2850 Loss 0.8075 Accuracy 0.5236\n",
            "Epoch 9 Batch 2900 Loss 0.8057 Accuracy 0.5239\n",
            "Epoch 9 Batch 2950 Loss 0.8039 Accuracy 0.5242\n",
            "Epoch 9 Batch 3000 Loss 0.8022 Accuracy 0.5244\n",
            "Epoch 9 Batch 3050 Loss 0.8005 Accuracy 0.5246\n",
            "Epoch 9 Batch 3100 Loss 0.7986 Accuracy 0.5248\n",
            "Epoch 9 Batch 3150 Loss 0.7970 Accuracy 0.5251\n",
            "Epoch 9 Batch 3200 Loss 0.7955 Accuracy 0.5253\n",
            "Epoch 9 Batch 3250 Loss 0.7942 Accuracy 0.5255\n",
            "Epoch 9 Batch 3300 Loss 0.7923 Accuracy 0.5258\n",
            "Epoch 9 Batch 3350 Loss 0.7902 Accuracy 0.5261\n",
            "Epoch 9 Batch 3400 Loss 0.7885 Accuracy 0.5262\n",
            "Epoch 9 Batch 3450 Loss 0.7870 Accuracy 0.5265\n",
            "Epoch 9 Batch 3500 Loss 0.7857 Accuracy 0.5267\n",
            "Epoch 9 Batch 3550 Loss 0.7841 Accuracy 0.5269\n",
            "Epoch 9 Batch 3600 Loss 0.7827 Accuracy 0.5273\n",
            "Epoch 9 Batch 3650 Loss 0.7811 Accuracy 0.5275\n",
            "Epoch 9 Batch 3700 Loss 0.7793 Accuracy 0.5278\n",
            "Epoch 9 Batch 3750 Loss 0.7779 Accuracy 0.5281\n",
            "Epoch 9 Batch 3800 Loss 0.7765 Accuracy 0.5284\n",
            "Epoch 9 Batch 3850 Loss 0.7752 Accuracy 0.5288\n",
            "Epoch 9 Batch 3900 Loss 0.7739 Accuracy 0.5290\n",
            "Epoch 9 Batch 3950 Loss 0.7727 Accuracy 0.5294\n",
            "Epoch 9 Batch 4000 Loss 0.7716 Accuracy 0.5296\n",
            "Epoch 9 Batch 4050 Loss 0.7702 Accuracy 0.5299\n",
            "Epoch 9 Batch 4100 Loss 0.7695 Accuracy 0.5301\n",
            "Epoch 9 Batch 4150 Loss 0.7690 Accuracy 0.5302\n",
            "Epoch 9 Batch 4200 Loss 0.7692 Accuracy 0.5302\n",
            "Epoch 9 Batch 4250 Loss 0.7698 Accuracy 0.5301\n",
            "Epoch 9 Batch 4300 Loss 0.7708 Accuracy 0.5300\n",
            "Epoch 9 Batch 4350 Loss 0.7716 Accuracy 0.5298\n",
            "Epoch 9 Batch 4400 Loss 0.7727 Accuracy 0.5297\n",
            "Epoch 9 Batch 4450 Loss 0.7741 Accuracy 0.5295\n",
            "Epoch 9 Batch 4500 Loss 0.7753 Accuracy 0.5293\n",
            "Epoch 9 Batch 4550 Loss 0.7766 Accuracy 0.5291\n",
            "Epoch 9 Batch 4600 Loss 0.7780 Accuracy 0.5289\n",
            "Epoch 9 Batch 4650 Loss 0.7792 Accuracy 0.5287\n",
            "Epoch 9 Batch 4700 Loss 0.7805 Accuracy 0.5286\n",
            "Epoch 9 Batch 4750 Loss 0.7817 Accuracy 0.5284\n",
            "Epoch 9 Batch 4800 Loss 0.7828 Accuracy 0.5282\n",
            "Epoch 9 Batch 4850 Loss 0.7841 Accuracy 0.5280\n",
            "Epoch 9 Batch 4900 Loss 0.7855 Accuracy 0.5278\n",
            "Epoch 9 Batch 4950 Loss 0.7869 Accuracy 0.5276\n",
            "Epoch 9 Batch 5000 Loss 0.7882 Accuracy 0.5273\n",
            "Epoch 9 Batch 5050 Loss 0.7897 Accuracy 0.5271\n",
            "Epoch 9 Batch 5100 Loss 0.7910 Accuracy 0.5269\n",
            "Epoch 9 Batch 5150 Loss 0.7924 Accuracy 0.5266\n",
            "Epoch 9 Batch 5200 Loss 0.7938 Accuracy 0.5263\n",
            "Epoch 9 Batch 5250 Loss 0.7950 Accuracy 0.5261\n",
            "Epoch 9 Batch 5300 Loss 0.7962 Accuracy 0.5258\n",
            "Epoch 9 Batch 5350 Loss 0.7974 Accuracy 0.5255\n",
            "Epoch 9 Batch 5400 Loss 0.7986 Accuracy 0.5252\n",
            "Epoch 9 Batch 5450 Loss 0.7997 Accuracy 0.5250\n",
            "Epoch 9 Batch 5500 Loss 0.8009 Accuracy 0.5246\n",
            "Epoch 9 Batch 5550 Loss 0.8019 Accuracy 0.5243\n",
            "Epoch 9 Batch 5600 Loss 0.8028 Accuracy 0.5241\n",
            "Epoch 9 Batch 5650 Loss 0.8039 Accuracy 0.5238\n",
            "Epoch 9 Batch 5700 Loss 0.8050 Accuracy 0.5235\n",
            "Saving checkpoint for epoch 9 at /content/drive/MyDrive/Projects/APM598 Project/Data/ckpt/ckpt-27\n",
            "Time taken for 1 epoch: 1567.4891810417175 secs\n",
            "\n",
            "Start of epoch 10\n",
            "Epoch 10 Batch 0 Loss 0.9258 Accuracy 0.4630\n",
            "Epoch 10 Batch 50 Loss 0.8998 Accuracy 0.5017\n",
            "Epoch 10 Batch 100 Loss 0.8972 Accuracy 0.5043\n",
            "Epoch 10 Batch 150 Loss 0.9080 Accuracy 0.5062\n",
            "Epoch 10 Batch 200 Loss 0.9032 Accuracy 0.5064\n",
            "Epoch 10 Batch 250 Loss 0.9007 Accuracy 0.5068\n",
            "Epoch 10 Batch 300 Loss 0.8995 Accuracy 0.5074\n",
            "Epoch 10 Batch 350 Loss 0.8988 Accuracy 0.5069\n",
            "Epoch 10 Batch 400 Loss 0.8978 Accuracy 0.5070\n",
            "Epoch 10 Batch 450 Loss 0.8947 Accuracy 0.5068\n",
            "Epoch 10 Batch 500 Loss 0.8935 Accuracy 0.5066\n",
            "Epoch 10 Batch 550 Loss 0.8938 Accuracy 0.5064\n",
            "Epoch 10 Batch 600 Loss 0.8914 Accuracy 0.5069\n",
            "Epoch 10 Batch 650 Loss 0.8923 Accuracy 0.5072\n",
            "Epoch 10 Batch 700 Loss 0.8915 Accuracy 0.5075\n",
            "Epoch 10 Batch 750 Loss 0.8905 Accuracy 0.5079\n",
            "Epoch 10 Batch 800 Loss 0.8895 Accuracy 0.5079\n",
            "Epoch 10 Batch 850 Loss 0.8891 Accuracy 0.5082\n",
            "Epoch 10 Batch 900 Loss 0.8897 Accuracy 0.5081\n",
            "Epoch 10 Batch 950 Loss 0.8880 Accuracy 0.5082\n",
            "Epoch 10 Batch 1000 Loss 0.8861 Accuracy 0.5081\n",
            "Epoch 10 Batch 1050 Loss 0.8851 Accuracy 0.5084\n",
            "Epoch 10 Batch 1100 Loss 0.8843 Accuracy 0.5084\n",
            "Epoch 10 Batch 1150 Loss 0.8832 Accuracy 0.5087\n",
            "Epoch 10 Batch 1200 Loss 0.8816 Accuracy 0.5090\n",
            "Epoch 10 Batch 1250 Loss 0.8790 Accuracy 0.5093\n",
            "Epoch 10 Batch 1300 Loss 0.8775 Accuracy 0.5100\n",
            "Epoch 10 Batch 1350 Loss 0.8751 Accuracy 0.5102\n",
            "Epoch 10 Batch 1400 Loss 0.8729 Accuracy 0.5106\n",
            "Epoch 10 Batch 1450 Loss 0.8708 Accuracy 0.5114\n",
            "Epoch 10 Batch 1500 Loss 0.8677 Accuracy 0.5120\n",
            "Epoch 10 Batch 1550 Loss 0.8652 Accuracy 0.5129\n",
            "Epoch 10 Batch 1600 Loss 0.8624 Accuracy 0.5136\n",
            "Epoch 10 Batch 1650 Loss 0.8601 Accuracy 0.5144\n",
            "Epoch 10 Batch 1700 Loss 0.8582 Accuracy 0.5153\n",
            "Epoch 10 Batch 1750 Loss 0.8556 Accuracy 0.5161\n",
            "Epoch 10 Batch 1800 Loss 0.8534 Accuracy 0.5170\n",
            "Epoch 10 Batch 1850 Loss 0.8513 Accuracy 0.5178\n",
            "Epoch 10 Batch 1900 Loss 0.8491 Accuracy 0.5185\n",
            "Epoch 10 Batch 1950 Loss 0.8475 Accuracy 0.5191\n",
            "Epoch 10 Batch 2000 Loss 0.8459 Accuracy 0.5197\n",
            "Epoch 10 Batch 2050 Loss 0.8440 Accuracy 0.5203\n",
            "Epoch 10 Batch 2100 Loss 0.8420 Accuracy 0.5206\n",
            "Epoch 10 Batch 2150 Loss 0.8395 Accuracy 0.5208\n",
            "Epoch 10 Batch 2200 Loss 0.8364 Accuracy 0.5209\n",
            "Epoch 10 Batch 2250 Loss 0.8333 Accuracy 0.5210\n",
            "Epoch 10 Batch 2300 Loss 0.8306 Accuracy 0.5213\n",
            "Epoch 10 Batch 2350 Loss 0.8278 Accuracy 0.5215\n",
            "Epoch 10 Batch 2400 Loss 0.8255 Accuracy 0.5217\n",
            "Epoch 10 Batch 2450 Loss 0.8226 Accuracy 0.5219\n",
            "Epoch 10 Batch 2500 Loss 0.8203 Accuracy 0.5222\n",
            "Epoch 10 Batch 2550 Loss 0.8179 Accuracy 0.5226\n",
            "Epoch 10 Batch 2600 Loss 0.8153 Accuracy 0.5228\n",
            "Epoch 10 Batch 2650 Loss 0.8127 Accuracy 0.5232\n",
            "Epoch 10 Batch 2700 Loss 0.8104 Accuracy 0.5235\n",
            "Epoch 10 Batch 2750 Loss 0.8084 Accuracy 0.5239\n",
            "Epoch 10 Batch 2800 Loss 0.8066 Accuracy 0.5241\n",
            "Epoch 10 Batch 2850 Loss 0.8045 Accuracy 0.5243\n",
            "Epoch 10 Batch 2900 Loss 0.8025 Accuracy 0.5246\n",
            "Epoch 10 Batch 2950 Loss 0.8006 Accuracy 0.5248\n",
            "Epoch 10 Batch 3000 Loss 0.7988 Accuracy 0.5250\n",
            "Epoch 10 Batch 3050 Loss 0.7969 Accuracy 0.5252\n",
            "Epoch 10 Batch 3100 Loss 0.7951 Accuracy 0.5254\n",
            "Epoch 10 Batch 3150 Loss 0.7933 Accuracy 0.5256\n",
            "Epoch 10 Batch 3200 Loss 0.7914 Accuracy 0.5259\n",
            "Epoch 10 Batch 3250 Loss 0.7897 Accuracy 0.5260\n",
            "Epoch 10 Batch 3300 Loss 0.7881 Accuracy 0.5262\n",
            "Epoch 10 Batch 3350 Loss 0.7866 Accuracy 0.5265\n",
            "Epoch 10 Batch 3400 Loss 0.7848 Accuracy 0.5267\n",
            "Epoch 10 Batch 3450 Loss 0.7834 Accuracy 0.5270\n",
            "Epoch 10 Batch 3500 Loss 0.7817 Accuracy 0.5272\n",
            "Epoch 10 Batch 3550 Loss 0.7799 Accuracy 0.5276\n",
            "Epoch 10 Batch 3600 Loss 0.7786 Accuracy 0.5278\n",
            "Epoch 10 Batch 3650 Loss 0.7772 Accuracy 0.5281\n",
            "Epoch 10 Batch 3700 Loss 0.7758 Accuracy 0.5283\n",
            "Epoch 10 Batch 3750 Loss 0.7744 Accuracy 0.5287\n",
            "Epoch 10 Batch 3800 Loss 0.7733 Accuracy 0.5290\n",
            "Epoch 10 Batch 3850 Loss 0.7720 Accuracy 0.5293\n",
            "Epoch 10 Batch 3900 Loss 0.7708 Accuracy 0.5296\n",
            "Epoch 10 Batch 3950 Loss 0.7696 Accuracy 0.5299\n",
            "Epoch 10 Batch 4000 Loss 0.7684 Accuracy 0.5302\n",
            "Epoch 10 Batch 4050 Loss 0.7673 Accuracy 0.5305\n",
            "Epoch 10 Batch 4100 Loss 0.7661 Accuracy 0.5307\n",
            "Epoch 10 Batch 4150 Loss 0.7661 Accuracy 0.5308\n",
            "Epoch 10 Batch 4200 Loss 0.7663 Accuracy 0.5308\n",
            "Epoch 10 Batch 4250 Loss 0.7668 Accuracy 0.5307\n",
            "Epoch 10 Batch 4300 Loss 0.7674 Accuracy 0.5306\n",
            "Epoch 10 Batch 4350 Loss 0.7682 Accuracy 0.5305\n",
            "Epoch 10 Batch 4400 Loss 0.7693 Accuracy 0.5303\n",
            "Epoch 10 Batch 4450 Loss 0.7708 Accuracy 0.5301\n",
            "Epoch 10 Batch 4500 Loss 0.7722 Accuracy 0.5299\n",
            "Epoch 10 Batch 4550 Loss 0.7733 Accuracy 0.5297\n",
            "Epoch 10 Batch 4600 Loss 0.7744 Accuracy 0.5295\n",
            "Epoch 10 Batch 4650 Loss 0.7758 Accuracy 0.5293\n",
            "Epoch 10 Batch 4700 Loss 0.7772 Accuracy 0.5291\n",
            "Epoch 10 Batch 4750 Loss 0.7784 Accuracy 0.5289\n",
            "Epoch 10 Batch 4800 Loss 0.7796 Accuracy 0.5287\n",
            "Epoch 10 Batch 4850 Loss 0.7808 Accuracy 0.5285\n",
            "Epoch 10 Batch 4900 Loss 0.7821 Accuracy 0.5283\n",
            "Epoch 10 Batch 4950 Loss 0.7833 Accuracy 0.5280\n",
            "Epoch 10 Batch 5000 Loss 0.7847 Accuracy 0.5278\n",
            "Epoch 10 Batch 5050 Loss 0.7861 Accuracy 0.5276\n",
            "Epoch 10 Batch 5100 Loss 0.7876 Accuracy 0.5274\n",
            "Epoch 10 Batch 5150 Loss 0.7889 Accuracy 0.5271\n",
            "Epoch 10 Batch 5200 Loss 0.7903 Accuracy 0.5269\n",
            "Epoch 10 Batch 5250 Loss 0.7917 Accuracy 0.5266\n",
            "Epoch 10 Batch 5300 Loss 0.7928 Accuracy 0.5263\n",
            "Epoch 10 Batch 5350 Loss 0.7941 Accuracy 0.5260\n",
            "Epoch 10 Batch 5400 Loss 0.7953 Accuracy 0.5258\n",
            "Epoch 10 Batch 5450 Loss 0.7963 Accuracy 0.5255\n",
            "Epoch 10 Batch 5500 Loss 0.7975 Accuracy 0.5252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_CRY8xrPTMW"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx0CvDQBSF_Y"
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        \n",
        "        prediction = predictions[:, -1:, :]\n",
        "        \n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        if predicted_id == VOCAB_SIZE_FR-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYt8Q4mXSH7g"
      },
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    \n",
        "    predicted_sentence = tokenizer_fr.decode(\n",
        "        [i for i in output if i < VOCAB_SIZE_FR-2]\n",
        "    )\n",
        "    \n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRGBzDBs-dfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "016b541c-1c4b-481c-c6de-3299d5ed902a"
      },
      "source": [
        "translate(\"The project for language translation\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: The project for language translation\n",
            "Predicted translation: Le projet de traduction des langues\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}